<!DOCTYPE html>
<html>
<head>
	<title>Book</title>
	<link href="{{ url_for('static', filename='bootstrap/bootstrap.css') }}" rel="stylesheet" type="text/css">
	<link href="{{ url_for('static', filename='CSS/bookPage.css') }}" rel="stylesheet" type="text/css">
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js">
	</script>
	<script>
	   $(document).ready(function(){
	   // Add smooth scrolling to all links
	   $("a").on('click', function(event) {

	       // Make sure this.hash has a value before overriding default behavior
	       if (this.hash !== "") {
	       // Prevent default anchor click behavior
	       event.preventDefault();

	       // Store hash
	       var hash = this.hash;

	       // Using jQuery's animate() method to add smooth page scroll
	       // The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
	       $('html, body').animate({
	           scrollTop: $(hash).offset().top
	       }, 800, function(){
	   
	           // Add hash (#) to URL when done scrolling (default click behavior)
	           window.location.hash = hash;
	       });
	       } // End if
	   });
	   });
	</script>
</head>
<body>
	<section class="app">
		<aside class="sidebar">
			<header>
				Twins
			</header>
			<nav class="sidebar-nav">
				<ul>
					<li>
						<a href="{{ url_for( 'index' ) }}"><i class="ion-bag"></i> <span>Home Page</span></a>
				  	</li>
					<li>
						<a href="{{ url_for( 'about', name='pito' ) }}"><i class="ion-ios-settings"></i> <span class="">About Pito</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'about', name='bart' ) }}"><i class="ion-ios-settings"></i> <span class="">About Bart</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'projectsPage' ) }}"><i class="ion-ios-settings"></i> <span class="">Projects Page</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'learningPage' ) }}"><i class="ion-ios-settings"></i> <span class="">Learning Page</span></a>
					</li>
					<li>
						<a href="#"><i class="ion-bag"></i> <span>Classification Algorithms</span></a>
						<ul class="nav-flyout">
							<li>
								<a href="#section1"><i class="ion-ios-color-filter-outline"></i>Decision Tree</a>
							</li>
							<li>
								<a href="#section2"><i class="ion-ios-clock-outline"></i>Naive Bayes</a>
							</li>
							<li>
								<a href="#section3"><i class="ion-ios-color-filter-outline"></i>KNN</a>
							</li>
							<li>
								<a href="#section4"><i class="ion-ios-clock-outline"></i>SVM</a>
							</li>
							<li>
								<a href="#section5"><i class="ion-ios-color-filter-outline"></i>SGD</a>
							</li>
							<li>
								<a href="#section6"><i class="ion-ios-clock-outline"></i>Logistic Regression</a>
							</li>
						</ul>
					</li>
    			</ul>
			</nav>
		</aside>
		<div id="section1">
			<br/><a class="topic" style="text-decoration: underline;">Decision Tree</a><br/>
			<br/><h2 class="basicText">Goal</h2>
			<p class="basicText">
				The goal is to create a model that predicts the value of a target variable based on several input variables. An example is shown in the 
				diagram. Each interior node corresponds to one of the input variables; there are edges to children for each of the possible 
				values of that input variable. Each leaf represents a value of the target variable given the values of the input variables represented 
				by the path from the root to the leaf. 
			</p>
			<h2 class="basicText">Split</h2>
			<p class="basicText">	
				In the first split or the root, all attributes/features are considered and the training data is divided into groups based on this split. 
				We have X features, so will have X candidate splits. Now we will calculate how much accuracy each split will cost us, using a function. 
				The split that costs least is chosen. This algorithm is recursive in nature as the groups formed can be sub-divided using same strategy. 
				Due to this procedure, this algorithm is also known as the greedy algorithm, as we have an excessive desire of lowering the cost. This 
				makes the root node as best predictor/classifier.
			</p>
			<img src="../static/views/DTree.png" alt="Decision Tree" style="margin-left: 20%">
		</div>
		<div id="section2">
			<a class="topic" style="text-decoration: underline;">Naive Bayes</a><br/>
			<br/><h2 class="basicText">Principle of Naive Bayes Classifier</h2>
			<p class="basicText">
				A Naive Bayes classifier is a probabilistic machine learning model that’s used for classification task. The crux of the classifier is 
				based on the Bayes theorem. 
			</p>
			<h2 class="basicText">Bayes Theorem</h2>
			<p class="basicText">	
				Using Bayes theorem, we can find the probability of A happening, given that B has occurred. Here, B is the evidence and A is the 
				hypothesis. The assumption made here is that the predictors/features are independent. That is presence of one particular feature does 
				not affect the other. Hence it is called naive. Another assumption made here is that all the predictors have an equal effect on the outcome.
			</p>
			<img src="../static/views/bayes.png" alt="bayes rule" style="margin-left: 20%; width:25%">
			<img src="../static/views/bayes2.png" alt="bayes rule" style="margin-left: 20%; width:25%">
		</div>	
		<div id="section3">
			<a class="topic" style="text-decoration: underline;">K-Nearest Neighbors</a><br/>
			<br/><h2 class="basicText">KNN steps:</h2>
			<p class="basicText">
				1. Receive an unclassified data;
				<br/>2. Measure the distance (Euclidian, Manhattan, Minkowski or Weighted) from the new data to all others data that is already classified;
				<br/>3. Gets the K(K is a parameter that you difine) smaller distances;
				<br/>4. Check the list of classes had the shortest distance and count the amount of each class that appears;
				<br/>5. Takes as correct class the class that appeared the most times;
				<br/>6. Classifies the new data with the class that you took in step 5;
			</p>
			<h2 class="basicText">Euclidean distance</h2>
			<p class="basicText">
				The Euclidean distance between points p and q is the length of the line segment connecting them.
			</p>
			<img src="../static/views/euclideanFormula.png" alt="euclidean Formula" style="margin-left: 20%; width:25%">
			<br/><br/><h2 class="basicText">What is K?</h2>
			<p class="basicText">
				K represents the number of training data points lying in proximity to the test data point which we are going to use to find the class.
			</p>
		</div>
		<div id="section4">
			<a class="topic" style="text-decoration: underline;">Support Vector Machine</a><br/>
			<br/><h2 class="basicText">Goal</h2>
			<p class="basicText">
				The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that 
				distinctly classifies the data points.
			</p>
			<br/><img src="../static/views/svm1.png" alt="svm" style="margin-left: 20%; width:25%"><br/>
			<br/><p class="basicText">
				To separate the two classes of data points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane 
				that has the maximum margin, i.e the maximum distance between data points of both classes. Maximizing the margin distance provides some 
				reinforcement so that future data points can be classified with more confidence.
			</p>
			<br/><p class="basicText">
					In machine learning, the hinge loss is a loss function used for training classifiers. The hinge loss is used for "maximum-margin" 
					classification, most notably for support vector machines.
			</p>
			<img src="../static/views/hingeLoss.png" alt="svm" style="margin-left: 20%; width:25%">
			<br/><br/><h2 class="basicText">Loss Function for SVM : </h2>
			<p class="basicText">
				Given this understanding of the hinge loss function for a SVM, lets add a regularization term (L2 norm) to the cost. The intuition behind the
				regularization term is that we increase the cost penalty if the values for the weights are high. So while trying to minimize the cost, we not
				only adjust the weights, we also try to minimize the value of the weights and thereby reduce over fitting to the training data and make the 
				model less sensitive to outliers. So with the added regularization term, the total cost function finally looks like:	
				<h2 class="basicText" style="margin-left: 22%;">Total cost = ||w²||/2 + C*(Sum of all losses for each observation)</h2>
			</p>
			<br/><br/><p class="basicText" style="font-weight: bold;">Two great articles about SVM:</p>
			<a href="https://towardsdatascience.com/support-vector-machines-intuitive-understanding-part-1-3fb049df4ba1" style="margin-left: 18%;">Support vector machines (intuitive understanding)</a>
			<br/><a href="https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47" style="margin-left: 18%;">Support Vector Machine — Introduction to Machine Learning Algorithms</a>
		</div>
		<div id="section5">
			<a class="topic" style="text-decoration: underline;">Stochastic Gradient Descent</a>
			<br/><h2 class="basicText">What is Gradient Descent?</h2><br/>
			<br/><p class="basicText">
				Gradient descent is an iterative machine learning optimization algorithm to reduce the cost function so that we have models that makes 
				accurate predictions. We get optimized weights using gradient descent. <br/><br/><span style="font-weight: bold;">Cost function(C) </span>or <span style="font-weight: bold;">Loss function </span> 
				measures the difference between the actual output and predicted output from the model. Cost function are a convex function.
			</p>
			<br/><h2 class="basicText">How can we find the optimized weights?</h2>
			<p class="basicText">
				We randomly initialize all the weights for a neural network to a value close to zero but not zero.
				<br/>We calculate the gradient, ∂c/∂ω which is a partial derivative of cost with respect to weight.
				<br/>α is learning rate, helps adjust the weights with respect to gradient descent. 
			</p>
			<br/><img src="../static/views/sgd1.png" alt="sgd" style="margin-left: 20%; width:25%"><br/>
			<br/><p class="basicText">
				We need to update the weights for all the neurons simultaneously.
			</p>
		</div>
		<div id="section6">
			<a class="topic" style="text-decoration: underline;">Logistic Regression</a>
		</div>
		</section>
</body>
</html>



  
