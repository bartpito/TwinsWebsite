<!DOCTYPE html></script>
<html>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
<head>
	<title>NLP</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link href="{{ url_for('static', filename='bootstrap/bootstrap.css') }}" rel="stylesheet" type="text/css">
	<link href="{{ url_for('static', filename='CSS/NLP.css') }}" rel="stylesheet" type="text/css">
	<script>
		$(document).ready(function(){
		// Add smooth scrolling to all links
		$("a").on('click', function(event) {
 
			// Make sure this.hash has a value before overriding default behavior
			if (this.hash !== "") {
			// Prevent default anchor click behavior
			event.preventDefault();
 
			// Store hash
			var hash = this.hash;
 
			// Using jQuery's animate() method to add smooth page scroll
			// The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
			$('html, body').animate({
				scrollTop: $(hash).offset().top
			}, 800, function(){
		
				// Add hash (#) to URL when done scrolling (default click behavior)
				window.location.hash = hash;
			});
			} // End if
		});
		});
	</script>
</head>
<body>
	<section class="app">
		<aside class="sidebar">
			<header style="font-weight: 999;" class="sidebar-navig">
				Natural Language Processing
			</header>
			<nav class="sidebar-nav">
				<ul>
					<li>
						<a href="#"><i class="ion-bag"></i> <span class="topic-decoration">&bull; Basic NLP</span></a>
						<ul class="nav-flyout">
							<li>
								<a href="#section0"><i class="ion-ios-color-filter-outline"></i>NLP pipeline</a>
							</li>
							<li>
								<a href="#section1"><i class="ion-ios-color-filter-outline"></i>Data Augmentation</a>
							</li>
							<li>
								<a href="#section2"><i class="ion-ios-clock-outline"></i>Tokenization</a>
							</li>
							<li>
								<a href="#section3"><i class="ion-ios-color-filter-outline"></i>Text Preprocessing</a>
							</li>
						 	<li>
								<a href="#section4"><i class="ion-ios-clock-outline"></i>Regular Expressions</a>
							</li>
						 	<li>
								<a href="#section5"><i class="ion-ios-color-filter-outline"></i>POS Tagging</a>
							</li>
							<li>
								<a href="#section6"><i class="ion-ios-clock-outline"></i>Stop Words</a>
							</li>
							<li>
								<a href="#section7"><i class="ion-ios-clock-outline"></i>Dependency Parsing</a>
							</li>
							<li>
								<a href="#section8"><i class="ion-ios-clock-outline"></i>NER</a>
							</li>
							<li>
								<a href="#section9"><i class="ion-ios-clock-outline"></i>Coreference Resolution</a>
							</li>
						</ul>
					</li>
					<li>
						<a href="#section10"><i class="ion-bag"></i> <span class="topic-decoration">&bull; NLTK (+BeautifulSoup, Inflect, Contractions)</span></a>
					</li>
					<li>
						<a href="#section11"><i class="ion-bag"></i> <span class="topic-decoration">&bull; SpaCy</span></a>
					</li>
					<li>
						<a href="#section12"><i class="ion-bag"></i> <span class="topic-decoration">&bull; Textblob</span></a>
					</li>
					<li>
						<a href="#section13"><i class="ion-bag"></i> <span class="topic-decoration">&bull; Word Vectors</span></a>
						<ul class="nav-flyout">
							<li>
								<a href="#section14"><i class="ion-ios-color-filter-outline"></i>Word2Vec</a>
							</li>
							<li>
								<a href="#section15"><i class="ion-ios-color-filter-outline"></i>GloVe</a>
							</li>
						</ul>
					</li>
				</ul>
				<ul>
					<h2></h2>
					<li>
						<a href="{{ url_for( 'index' ) }}"><i class="ion-bag"></i> <span style="color: #00FF00">&#8227 Home Page</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'about', name='pito' ) }}"><i class="ion-ios-settings"></i> <span style="color: #00FF00">&#8227 About Pito</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'about', name='bart' ) }}"><i class="ion-ios-settings"></i> <span style="color: #00FF00">&#8227 About Bart</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'projectsPage' ) }}"><i class="ion-ios-settings"></i> <span style="color: #00FF00">&#8227 Projects Page</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'learningPage' ) }}"><i class="ion-ios-settings"></i> <span style="color: #00FF00">&#8227 Learning Page</span></a>
					</li>
				</ul>
			</nav>
		</aside>
		<div id="section0">
			<br/><a class="topic" style="text-decoration: underline;">NLP pipeline</a><br/>
			<br/><br/><img img class='imgPos' src="../static/views/pipl1.png" alt="cr1" style="margin-left: 20%">
			<br/><br/><p class="basicText2">
				Coreference resolution is an optional step that isn’t always done.	
				<br/><br/>it’s worth mentioning that these are the steps in a typical NLP pipeline, but you will skip steps or re-order steps depending on what you want to do and how your NLP library is implemented. For example, some libraries like spaCy do sentence segmentation much later in the pipeline using the results of the 
				dependency parse.
			</p>
		</div>
		<div id="section1">
			<br/><a class="topic" style="text-decoration: underline;">Data Augmentation</a><br/>
			<br/><h2 class="basicText">1. Easiest Data Augmentation Techniques in Natural Language Processing:</h2>
			<p class="basicText2">
				<br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">Synonym Replacement:</span><br/> Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.
				<br/><br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">Random Insertion:</span> <br/>Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times.
				<br/><br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">Random Swap:</span> <br/>Randomly choose two words in the sentence and swap their positions. Do this n times.
				<br/><br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">Random Deletion:</span> <br/>Randomly remove each word in the sentence with probability p.
				<br/><br/><a href="https://github.com/jasonwei20/eda_nlp/blob/master/code/eda.py" style="font-weight: bold; text-decoration: underline; color: black; font-size: 1.2vw;">Code Implementation</a>
			</p>
		</div>
		<div id="section2">
			<br/><a class="topic" style="text-decoration: underline;">Tokenization</a><br/>
			<br/><h2 class="basicText">1. Description:</h2>
			<p class="basicText2">
				<br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">The process of segmenting running text into words and sentences.</span>
				Electronic text is a linear sequence of symbols (characters or words or phrases). Naturally, before any real text processing is to be done, text needs to be segmented into linguistic units such as words, punctuation, numbers, alpha-numerics, etc. This process is called tokenization.
				In English, words are often separated from each other by blanks (white space), but not all white space is equal. Both “Los Angeles” and “rock 'n' roll” are individual thoughts despite the fact that they contain multiple words and spaces. We may also need to separate single words like “I'm” into separate words “I” and “am”.
				Tokenization is a kind of pre-processing in a sense; an identification of basic units to be processed. It is conventional to concentrate on pure analysis or generation while taking basic units for granted. Yet without these basic units clearly segregated it is impossible to carry out any analysis or generation.
				The identification of units that do not need to be further decomposed for subsequent processing is an extremely important one. Errors made at this stage are very likely to induce more errors at later stages of text processing and are therefore very dangerous.
				<br/><br/><br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">More information can be found at this link:</span>
				<br/><br/><a href="https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en" style="font-weight: bold; text-decoration: underline; color: black; font-size: 1.2vw;">The Art of Tokenization</a>
			</p>
		</div>
		<div id="section3">
			<br/><a class="topic" style="text-decoration: underline;">Text Preprocessing</a><br/>

			<br/><h2 class="basicText">What is text preprocessing?</h2>
			<p class="basicText2">
				<br/>&bull; To preprocess your text simply means to bring your text into a form that is <span style="font-weight: bold; font-size: 1.0vw;">predictable</span> and <span style="font-weight: bold; font-size: 1.0vw;">analyzable</span> for your task. A task here is a combination of approach and domain. For example, extracting 
				top keywords with TF-IDF (approach) from Tweets (domain) is an example of a Task.
			</p>
			<br/><h2 class="basicText">Types of text preprocessing techniques:</h2>
			<p class="basicText2">
				<span style="font-weight: bold; font-size: 1.1vw;">&bull; Lowercasing</span>
				<br/>Lowercasing ALL your text data, although commonly overlooked, is one of the simplest and most effective form of text preprocessing. It is applicable to most text mining and NLP problems and can help in cases where your dataset is not very large and significantly helps with consistency of expected output.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Stemming</span>
				<br/>Stemming is the process of reducing inflection in words (e.g. troubled, troubles) to their root form (e.g. trouble). The “root” in this case may not be a real root word, but just a canonical form of the original word.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Lemmatization</span>
				<br/>Lemmatization on the surface is very similar to stemming, where the goal is to remove inflections and map a word to its root form. The only difference is that, lemmatization tries to do it the proper way. It doesn’t just chop things off, it actually transforms words to the actual root. For example, the word “better”
				would map to “good”. It may use a dictionary such as WordNet for mappings or some special rule-based approaches.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Stopword Removal</span>
				<br/>Stop words are a set of commonly used words in a language. Examples of stop words in English are “a”, “the”, “is”, “are” and etc. The intuition behind using stop words is that, by removing low information words from text, we can focus on the important words instead.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Normalization</span>
				<br/>A highly overlooked preprocessing step is text normalization. Text normalization is the process of transforming a text into a canonical (standard) form. For example, the word “gooood” and “gud” can be transformed to “good”, its canonical form. Another example is mapping of near identical words such as “stopwords”, 
				“stop-words” and “stop words” to just “stopwords”.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Noise Removal</span>
				<br/>Noise removal is about removing characters digits and pieces of text that can interfere with your text analysis. Noise removal is one of the most essential text preprocessing steps. It is also highly domain dependent.
				<br/><br/>For example, in Tweets, noise could be all special characters except hashtags as it signifies concepts that can characterize a Tweet. The problem with noise is that it can produce results that are inconsistent in your downstream tasks.
				<br/><br/><br/>&bull; <span style="font-weight: bold; font-size: 1.1vw;">More information can be found at this link:</span>
				<br/><br/><a href="https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html" style="font-weight: bold; text-decoration: underline; color: black; font-size: 1.2vw;">All you need to know about text preprocessing for NLP and Machine Learning</a>
			</p>
		</div>
		<div id="section4">
			<br/><a class="topic" style="text-decoration: underline;">Regular Expressions</a><br/>
			<br/><h2 class="basicText">Basic Topics:</h2>
			<p class="basicText2">
				<br/><span style="font-weight: bold; font-size: 1.2vw;">Anchors:&nbsp;&nbsp;  ^ and $  </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">^The</span>matches any string that starts with The
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">end$</span>matches a string that ends with end
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">^The end$</span>exact string match (starts and ends with The end)
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">roar</span>matches any string that has the text roar in it
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Quantifiers:&nbsp;&nbsp; * + ? and {} </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc*</span>matches a string that has ab followed by zero or more c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc+</span>matches a string that has ab followed by one or more c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc?</span>matches a string that has ab followed by zero or one c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc{2}</span>matches a string that has ab followed by 2 c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc{2,}</span>matches a string that has ab followed by 2 or more c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc{2,5}</span>matches a string that has ab followed by 2 up to 5 c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(bc)*</span>matches a string that has a followed by zero or more copies of the sequence bc
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(bc){2,5}</span>matches a string that has a followed by 2 up to 5 copies of the sequence bc
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">OR operator:&nbsp;&nbsp; | or [] </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(b|c)</span>matches a string that has a followed by b or c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a[bc]</span>same as previous
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Character Classes:&nbsp;&nbsp;  \d \w \s and .</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\d</span>matches a single character that is a digit
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\w</span>matches a word character (alphanumeric character plus underscore)
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\s</span>matches a whitespace character (includes tabs and line breaks)
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">.</span>matches any character
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\D</span>matches a single non-digit character
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\W</span>matches any non-word character (equal to [^a-zA-Z0-9_])
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\S</span>matches any non-whitespace character (equal to [^\r\n\t\f\v ])
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw; padding-right: 5em; margin-left:2%;">In order to be taken literally, you must escape the characters ^.[$()|*+?{\with a backslash \ as they have special meaning.</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\$\d:&nbsp;&nbsp;</span>matches a string that has a $ before one digit
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Flags </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">g</span>(global) does not return after the first match, restarting the subsequent searches from the end of the previous match
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">m</span>(multi-line) when enabled ^ and $ will match the start and end of a line, instead of the whole string
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">i</span>(insensitive) makes the whole expression case-insensitive (for instance /aBc/i would match AbC)
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Grouping and Capturing:&nbsp;&nbsp; () </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(bc)</span>parentheses create a capturing group with value bc
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(?:bc)*</span>using ?: we disable the capturing group
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(?<foo>bc)</span>using ?<foo> we put a name to the group
			</p>
			<p class="basicText2">
				<br/><span style="font-weight: bold; font-size: 1.2vw;">Bracket Expressions:&nbsp;&nbsp;  []</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">[abc]</span>matches a string that has either an a or a b or a c 
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">[a-c]</span>same as previous
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">[a-fA-F0-9]</span>a string that represents a single hexadecimal digit, case insensitively
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">[0-9]%</span>a string that has a character from 0 to 9 before a % sign
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">[^a-zA-Z]</span>a string that has not a letter from a to z or from A to Z. In this case the ^ is used as negation of the expression
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">Remember that inside bracket expressions all special characters (including the backslash \) lose their special powers: thus we will not apply the “escape rule”.</span>
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Greedy and Lazy match:</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;"><.+?></span> matches any character one or more times included inside < and >, expanding as needed
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;"><[^<>]+> =</span>matches any character except < or > one or more times included inside < and >
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Boundaries:&nbsp;&nbsp;  \b and \B</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\babc\b</span> performs a "whole words only"</and>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\Babc\B</span>matches only if the pattern is fully surrounded by word characters
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Back-references:&nbsp;&nbsp; \1 </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">([abc])\1</span> using \1 it matches the same text that was matched by the first capturing group
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">([abc])([de])\2\1 </span> we can use \2 (\3, \4, etc.) to identify the same text that was matched by the second (third, fourth, etc.) capturing group
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">(?<foo>[abc])\k<foo></foo></span>we put the name foo to the group and we reference it later (\k<foo>). The result is the same of the first regex
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Look-ahead and Look-behind:&nbsp;&nbsp; (?=) and (?&lt;=) </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">d(?=r)</span>matches a d only if is followed by r, but r will not be part of the overall regex match
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">(?&lt;=r)d</span>matches a d only if is preceded by an r, but r will not be part of the overall regex match
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">d(?!r)</span>matches a d only if is not followed by r, but r will not be part of the overall regex match
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">(?&lt;!r)d</span>matches a d only if is not preceded by an r, but r will not be part of the overall regex match
			</p>
		</div>
		<div id="section5">
			<br/><a class="topic" style="text-decoration: underline;">Part-Of-Speech (POS) Tagging</a><br/>
			<br/><h2 class="basicText">Generative VS Discriminative Models:</h2>
			<p class="basicText2">
				<br/><br/><img img class='imgPos' src="../static/views/gvsd.png" alt="gen vs disc model" style="margin-left: 20%">
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Generative Algorithm</span> models how the data was generated in order to categorize a signal. It asks the question: based on my generation 
				assumptions, which category is most likely to generate this signal?
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Discriminative Algorithm</span> does not care about how the data was generated, it simply categorizes a given signal.
			</p>
			<br/><h2 class="basicText">Goal:</h2>
			<p class="basicText2">
				<br/>POS tagging is the process of marking up a word in a corpus to a corresponding part of a speech tag, based on its context and definition.
			</p>
			<br/><br/><h2 class="basicText">The Different POS Tagging Techniques</h2>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Lexical Based Methods</span>  Assigns the POS tag the most frequently occurring with a word in the training corpus.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Rule-Based Methods</span> Assigns POS tags based on rules. For example, we can have a rule that says, words ending 
				with “ed” or “ing” must be assigned to a verb. Rule-Based Techniques can be used along with Lexical Based approaches to allow POS Tagging of words that are not present in the training corpus but are there in the testing data.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Probabilistic Methods</span>  This method assigns the POS tags based on the probability of a particular tag sequence occurring. Conditional Random Fields (CRFs) and Hidden Markov Models (HMMs) are probabilistic approaches to assign a POS Tag.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Deep Learning Methods</span>  Recurrent Neural Networks can also be used for POS tagging.
			</p>
		</div>
		<div id="section6">
			<br/><a class="topic" style="text-decoration: underline;">Stop Words</a><br/>
			<br/><h2 class="basicText">&bull; What are stop words?</h2>
			<p class="basicText2">
				<span style="font-weight: bold; font-size: 1.0vw;">“Stop words”</span> are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts.
			</p>
			<br/><br/><h2 class="basicText">&bull; when should I remove stop words?</h2>
			<br/><p class="basicText2">
				Problems like sentiment analysis are much more sensitive to stop words removal than document classification.
				<br/><br/>You should remove these tokens only if they don’t add any new information for your problem. Classification problems normally don’t need stop words because it’s possible to talk about the general idea of a text even if you 
				remove stop words from it.
				<br/><br/>So, for theme classification, stop words are useless. In any other case, it’s better to keep these words and do some tests with and without them so see how it affects the model. Anyway, you should <span style="font-weight: bold; font-size: 1.0vw;">never remove stop
				words without thinking about the impact of these words on the problem you are trying to solve.</span>
			</p>
		</div>
		<div id="section7">
			<br/><a class="topic" style="text-decoration: underline;">Dependency Parsing</a><br/>
			<br/><h2 class="basicText">&bull; What is Dependency parsing?</h2>
			<br/><p class="basicText2">
				Dependency parsing figure out how all the words in a sentence relate to each other.
			</p>
			<br/><h2 class="basicText">&bull; GOAL:</h2>
			<p class="basicText2">
					The goal is to build a tree that assigns a single parent word to each word in the sentence. The root of the tree will be the main verb in the sentence. 
			</p>
			<br/><br/><img img class='imgPos' src="../static/views/dp1.png" alt="dp1" style="margin-left: 20%">
			<br/><br/><p class="basicText2">
				But we can go one step further. In addition to identifying the parent word of each word, we can also predict the type of relationship that exists between those two words:
				<br/><br/><img img class='imgPos' src="../static/views/dp2.png" alt="dp2" style="margin-left: 20%">
			</p>
			<br/><br/><h2 class="basicText">&bull; Finding Noun Phrases:</h2>
			<p class="basicText2">
				<br/>So far, we’ve treated every word in our sentence as a separate entity. But sometimes it makes more sense to group together the words that represent a single idea or thing. We can use the information from the dependency parse tree to automatically group together words that are all talking about the same thing.
				<br/><br/>For example, instead of this:
				<br/><br/><img img class='imgPos' src="../static/views/dp3.png" alt="dp1" style="margin-left: 20%">
				<br/><br/>We can group the noun phrases to generate this:
				<br/><br/><img img class='imgPos' src="../static/views/dp4.png" alt="dp1" style="margin-left: 20%">
				<br/><br/>Whether or not we do this step depends on our end goal. But it’s often a quick and easy way to simplify the sentence if we don’t need extra detail about which words are adjectives and instead care more about extracting complete ideas.
			</p>
		</div>
		<div id="section8">
			<br/><a class="topic" style="text-decoration: underline;">Named Entity Recognition (NER)</a><br/>
			<br/><h2 class="basicText">&bull; Description</h2>
			<p class="basicText2">
				In our sentence, we have the following nouns:
				<br/><br/><img img class='imgPos' src="../static/views/ner1.png" alt="dp1" style="margin-left: 20%">
				<br/><br/>Some of these nouns present real things in the world. For example, “London”, “England” and “United Kingdom” represent physical places on a map. It would be nice to be able to detect that! With that information, we could automatically extract a list of real-world places mentioned in a document using NLP.
				<br/><br/>The goal of Named Entity Recognition, or NER, is to detect and label these nouns with the real-world concepts that they represent. Here’s what our sentence looks like after running each token through our NER tagging model:
				<br/><br/><img img class='imgPos' src="../static/views/ner2.png" alt="dp1" style="margin-left: 20%">
				<br/><br/>But NER systems aren’t just doing a simple dictionary lookup. Instead, they are using the context of how a word appears in the sentence and a statistical model to guess which type of noun a word represents. A good NER system can tell the difference between “Brooklyn Decker” the person and the place “Brooklyn” 
				using context clues.
				<br/><br/>Here are just some of the kinds of objects that a typical NER system can tag:
				<br/><br/><ul class="basicText2">
					<li>People’s names</li>
					<li>Company names</li>
					<li>Geographic locations (Both physical and political)</li>
					<li>Product names</li>
					<li>Dates and times</li>
					<li>Amounts of money</li>
					<li>Names of events</li>
				</ul>
			</p>
		</div>
		<div id="section9">
			<br/><a class="topic" style="text-decoration: underline;">Coreference Resolution</a><br/>
			<br/><h2 class="basicText">&bull; Description</h2>
			<p class="basicText2">
				<br/>English is full of pronouns — words like he, she, and it. These are shortcuts that we use instead of writing out names over and over in each sentence. Humans can keep track of what these words represent based on context. But our NLP model doesn’t know what pronouns mean because it only examines one sentence at a time.
				<br/><br/>Let’s look at the third sentence in our document:
				<br/><br/>“It was founded by the Romans, who named it Londinium.”
				<br/><br/>If we parse this with our NLP pipeline, we’ll know that “it” was founded by Romans. But it’s a lot more useful to know that “London” was founded by Romans.
				<br/><br/><img img class='imgPos' src="../static/views/cr1.png" alt="cr1" style="margin-left: 20%">
			</p>
		</div>
		<div id="section10">
			<br/><a class="topic" style="text-decoration: underline;">NLTK</a><br/>
			<br/><br/><img img class='imgPos' src="../static/views/nltk1.png" alt="nltk" style="margin-left: 20%">
			<img img class='imgPos' src="../static/views/nltk2.png" alt="nltk" style="margin-left: 20%">
		</div>
		<div id="section11">
			<br/><a class="topic" style="text-decoration: underline;">SpaCy</a><br/>
			<br/><br/><img img class='imgPos' src="../static/views/spacy1.png" alt="spaCy" style="margin-left: 20%">
			<img img class='imgPos' src="../static/views/spacy2.png" alt="spaCy" style="margin-left: 20%">
		</div>
		<div id="section12">
			<br/><a class="topic" style="text-decoration: underline;">Textblob</a><br/>
			<br/><br/><img img class='imgPos' src="../static/views/textblob.png" alt="textblob" style="margin-left: 20%">
		</div>
		<div id="section13">
			<br/><a class="topic" style="text-decoration: underline;">Word Representation in Natural Language Processing</a><br/>
			<br/><h2 class="basicText">&bull; Dictionary Lookup:</h2>
			<br/><p class="basicText2">
				First, take the corpus which can be collection of words, sentences or texts. Pre-process them into an intended format. One way is to use lemmatization, which is a process of converting word to its base form. For example, given words walk, walking, walks and walked, their lemma would be walk.
				<br/><br/>After that, build the lookup dictionary by creating a mapping between words and IDs i.e. each unique word in the vocabulary is assigned an ID.
				<br/><br/>Then, for each given word, return the corresponding integer representation by looking it up in the dictionary. If the word is not present in the dictionary, the integer corresponding to the Out of Vocabulary token should be returned.
				<br/><br/>By treating tokens as integers, the model might incorrectly assume the existence of natural ordering. For example, the dictionary contains entries such as 1: “airport” and 2: “plane” . The token with greater ID value might be considered as more important by the Deep learning models than the tokens 
				with less values which is a wrong assumption. Models which are trained with this type of data are prone to failure. On the contrary, data with ordinal values such as size measures 1: “small”, 2: “medium”, 3:“large” is suitable for this case. Because there is a natural ordering in the data.
			</p>
			<br/><h2 class="basicText">&bull; One-Hot Encoding</h2>
			<br/><p class="basicText2">
				The second approach of word representation is one-hot encoding. The main idea is to create a vocabulary size vector with filled zeros except one. For a single word only corresponding column is filled with the value 1 and the rest are zero valued. The encoded tokens will consist of vector with dimension
				1 × (N+ 1), where N is the size of the dictionary and the extra 1 is added to N for the Out of Vocabulary token.
				The advantage of this encoding to ordinal representation is that it does not suffer from undesirable bias. However, its immense and sparse vector representation requires large memory for computation.
				<br/><br/><img img class='imgPos' src="../static/views/onehot1.jpeg" alt="onehot">
			</p>
			<br/><h2 class="basicText">&bull; Distributional Representation</h2>
			<br/><p class="basicText2">
				The main idea behind this approach is that words typically appearing in the similar context would have a similar meaning. The idea is to store the word-context co-occurrence matrix F in which rows represent words in the vocabulary and columns represent contexts. The context could be sliding windows over
				the training sentences, or even documents. The matrix entries consist a frequency counts or tf-idf (Term Frequency-Inverse Document Frequency) scores.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">Sentence 1:</span> Boston has available flights to major US cities.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">Sentence 2:</span> Flights to Boston were cancelled due to bad weather conditions.
				<br/><br/><img img class='imgPos' src="../static/views/dpr1.png" alt="dpr1">
				<br/><br/>Since the number of context could be very large, e.g. document might contain thousand of sentences, these methods are known for being inefficient.
			</p>
			<br/><h2 class="basicText">&bull; Word Embedding</h2>
			<p class="basicText2">
				Collective term for models that learned to map a set of words or phrases in a vocabulary to vectors of numerical values.Word Embedding is really all about improving the ability of networks to learn from text data. By representing that data as lower dimensional vectors. These vectors are called Embedding.
			</p>
			<br/><h2 class="basicText">&bull; How it is done?</h2>
			<br/><p class="basicText2">
				General approach for dealing with words in your text data is to one-hot encode your text. You will have tens of thousands of unique words in your text vocabulary. Computations with such one-hot encoded vectors for these words will be very inefficient because most values in your one-hot vector will be 0. 
				So, the matrix calculation that will happen in between a one-hot vector and a first hidden layer will result in a output that will have mostly 0 values
				<br/><br/><img img class='imgPos' src="../static/views/wv2.png" alt="wv">
				<br/><br/>Now, instead of doing the matrix multiplication between the inputs and hidden layer we directly grab the values from embedding weight matrix. We can do this because the multiplication of one-hot vector with weight matrix returns the row of the matrix corresponding to the index of ‘1’ input unit
				<br/><br/><img img class='imgPos' src="../static/views/wv3.png" alt="wv">
				<br/><br/>So, we use this Weight Matrix as lookup table. We encode the words as integers, for example ‘cool’ is encoded as 512, ‘hot’ is encoded as 764. Then to get hidden layer output value for ‘cool’ we just simply need to lookup the 512th row in the weight matrix. This process is called Embedding Lookup. 
				The number of dimension in the hidden layer output is the embedding dimension
				<br/><br/><img img class='imgPos' src="../static/views/wv4.png" alt="wv">
				<span style="font-weight: bold; font-size: 1.0vw;">Summary</span>
				<br/><br/>&bull; The embedding layer is just a hidden layer
				<br/><br/>&bull; The lookup table is just a embedding weight matrix
				<br/><br/>&bull; The lookup is just a shortcut for matrix multiplication
				<br/><br/>&bull; The lookup table is trained just like any weight matrix
			</p>
		</div>
		<div id="section14">
			<br/><a class="topic" style="text-decoration: underline;">Word2Vec</a><br/>
			<br/><p class="basicText2">
				<span style="font-weight: bold; font-size: 1.0vw;">Introduction</span>
				<br/>Word2Vec model is used for learning vector representations of words called “word embeddings”. This is typically done as a preprocessing step, after which the learned vectors are fed into a discriminative model (typically an RNN) to generate predictions and perform all sort of interesting things.
				<br/><br/><br/><img img class='imgPos' src="../static/views/dvg1.png" alt="dvg">
				<br/><br/><br/><br/><span style="font-weight: bold; font-size: 1.0vw;">Co-occurrence matrix</span>
				<br/><br/>A co-occurrence matrix is a matrix that contains the number of counts of each word appearing next to all the other words in the corpus (or training set).
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">"I love NLP and I like dogs"</span>
				<br/><br/><br/><img img class='imgPos' src="../static/views/com.png" alt="co-occurrence">
				<br/><br/><br/><img img class='imgPos' src="../static/views/wv5.png" alt="wv">
				<br/><br/>Notice that through this simple matrix, we’re able to gain pretty useful insights. For example, notice that the words ‘love’ and ‘like’ both contain 1’s for their counts with nouns (NLP and dogs). They also have 1’s for the count with “I”, thus indicating that the words must be some sort of verb. 
				With a larger dataset than just one sentence, you can imagine that this similarity will become more clear as ‘like’, ‘love’, and other synonyms will begin to have similar word vectors, because of the fact that they are used in similar contexts.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">Now, although this a great starting point, we notice that the dimensionality of each word will increase linearly with the size of the corpus.</span> If we had a million words (not really a lot in NLP standards), we’d have a million by million sized matrix which would be extremely sparse (lots of 0’s). Definitely 
				not the best in terms of storage efficiency. There have been numerous advancements in finding the most optimal ways to represent these word vectors. The most famous of which is Word2Vec.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">How does Word2Vec work?</span>
				<br/>The algorithm exists in two flavors <span style="font-weight: bold; font-size: 1.0vw;">CBOW and Skip-Gram.</span> Given a set of sentences (also called corpus) the model loops on the words of each sentence and either tries to use the current word of to predict its neighbors (its context), in which case the method is called “Skip-Gram”, or it uses each of these contexts 
				to predict the current word, in which case the￼ method is called “Continuous Bag Of Words” (CBOW). The limit on the number of words in each context is determined by a parameter called “window size”.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">Skip-gram model:</span>
				<br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Intuition</span>
				<br/><br/>The skip-gram neural network model is actually surprisingly simple in its most basic form. Train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.
				<br/><br/>We’re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.
				<br/><br/>The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if you gave the trained network the input word “Soviet”, the output probabilities are going to be much higher for words like “Union” and “Russia” than for unrelated words like “watermelon” and “kangaroo”.
				<br/><br/>We’ll train the neural network to do this by feeding it word pairs found in our training documents. The below example shows some of the training samples (word pairs) we would take from the sentence “The quick brown fox jumps over the lazy dog.” I’ve used a small window size of 2 just for the example. The word highlighted in blue is the input word.
				<br/><br/><br/><img img class='imgPos' src="../static/views/sg1.png" alt="sg">
				<br/><br/>We’re going to represent an input word like “ants” as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) and we’ll place a “1” in the position corresponding to the word “ants”, and 0s in all of the other positions.
				<br/><br/>The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.
				<br/><br/><br/><img img class='imgPos' src="../static/views/sg2.png" alt="sg">
				<br/><br/>There is no activation function on the hidden layer neurons, but the output neurons use softmax.
				<br/><br/>For our example, we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).
				<br/><br/>300 features is what Google used in their published model trained on the Google news dataset (you can download it from here). The number of features is a “hyper parameter” that you would just have to tune to your application (that is, try different values and see what yields the best results).
				<br/><br/>If you look at the rows of this weight matrix, these are actually what will be our word vectors!
				<br/><br/><br/><img img class='imgPos' src="../static/views/sg3.png" alt="sg">
				<br/><br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Improvement</span>
				<br/><br/>We need few additional modifications to the basic skip-gram model which are important for actually making it feasible to train. Running gradient descent on a neural network that large is going to be slow. And to make matters worse, you need a huge amount of training data in order to tune that many weights and avoid over-fitting. millions of weights times billions of training samples means that training this model is going to be a beast. The authors of Word2Vec addressed these issues in their second paper.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">1. Treating common word pairs or phrases as single “words” in their model.</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">2. Subsampling frequent words to decrease the number of training examples.</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">3. Modifying the optimization objective with a technique they called “Negative Sampling”, which causes each training sample to update only a small percentage of the model’s weights.</span>
				<br/><br/>It’s worth noting that subsampling frequent words and applying Negative Sampling not only reduced the compute burden of the training process, but also improved the quality of their resulting word vectors as well.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">Subsampling: </span>
				<br/><br/>There are two “problems” with common words like “the”:
				<br/><br/>When looking at word pairs, (“fox”, “the”) doesn’t tell us much about the meaning of “fox”. “the” appears in the context of pretty much every word.
				<br/><br/>We will have many more samples of (“the”, …) than we need to learn a good vector for “the”.
				<br/><br/>Word2Vec implements a “subsampling” scheme to address this. For each word we encounter in our training text, there is a chance that we will effectively delete it from the text. The probability that we cut the word is related to the word’s frequency.
				<br/><br/>If we have a window size of 10, and we remove a specific instance of “the” from our text:
				<br/><br/>As we train on the remaining words, “the” will not appear in any of their context windows.
				<br/><br/>We’ll have 10 fewer training samples where “the” is the input word.
				<br/><br/>Note how these two effects help address the two problems stated above.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">Negative Sampling:</span>
				<br/><br/>As we discussed above, the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples!
				<br/><br/>Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Here’s how it works.
				<br/><br/>When training the network on the word pair (“fox”, “quick”), recall that the “label” or “correct output” of the network is a one-hot vector. That is, for the output neuron corresponding to “quick” to output a 1, and for all of the other thousands of output neurons to output a 0.
				<br/><br/>With negative sampling, we are instead going to randomly select just a small number of “negative” words (let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).
				<br/><br/>The paper says that selecting 5–20 words works well for smaller datasets, and you can get away with only 2–5 words for large datasets.
				<br/><br/>Recall that the output layer of our model has a weight matrix that’s 300 x 10,000. So we will just be updating the weights for our positive word (“quick”), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!
				<br/><br/>In the hidden layer, only the weights for the input word are updated (this is true whether you’re using Negative Sampling or not).
				<br/><br/>The “negative samples” (that is, the 5 output words that we’ll train to output 0) are chosen using a “unigram distribution”.
				<br/><br/>Essentially, the probability for selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">The Continuous Bag of Words (CBOW) Model:</span>
				<br/>The CBOW model architecture tries to predict the current target word (the center word) based on the source context words (surrounding words). Considering a simple sentence, “the quick brown fox jumps over the lazy dog”, this can be pairs of (context_window, target_word) where if we consider a context window of size 2, we have examples like ([quick, fox], 
				brown), ([the, brown], quick), ([the, dog], lazy) and so on. Thus the model tries to predict the target_word based on the context_window words.
				<br/><br/><br/><img img class='imgPos' src="../static/views/cbow2.png" alt="cbow">
				<br/><br/>The input or the context word is a one hot encoded vector of size V. The hidden layer contains N neurons and the output is again a V length vector with the elements being the softmax values.
				<br/><br/>Let’s get the terms in the picture right:
				<br/><br/>- Wvn is the weight matrix that maps the input x to the hidden layer (V*N dimensional matrix)
				<br/><br/>-W`nv is the weight matrix that maps the hidden layer outputs to the final output layer (N*V dimensional matrix)
				<br/><br/>The hidden layer neurons just copy the weighted sum of inputs to the next layer. There is no activation like sigmoid, tanh or ReLU. The only non-linearity is the softmax calculations in the output layer.
				<br/><br/>But, the above model used a single context word to predict the target. We can use multiple context words to do the same.
				<br/><br/><br/><img img class='imgPos' src="../static/views/cbow1.png" alt="cbow">
				<br/><br/>The above model takes C context words. When Wvn is used to calculate hidden layer inputs, we take an average over all these C context word inputs.
			</p>
		</div>
		<div id="section15">
			<br/><a class="topic" style="text-decoration: underline;">GloVe</a><br/>
	</section>
</body>
</html>
