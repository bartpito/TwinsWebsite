<!DOCTYPE html></script>
<html>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
<head>
	<title>NLP</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link href="{{ url_for('static', filename='bootstrap/bootstrap.css') }}" rel="stylesheet" type="text/css">
	<link href="{{ url_for('static', filename='CSS/NLP.css') }}" rel="stylesheet" type="text/css">
	<script>
		$(document).ready(function(){
		// Add smooth scrolling to all links
		$("a").on('click', function(event) {
 
			// Make sure this.hash has a value before overriding default behavior
			if (this.hash !== "") {
			// Prevent default anchor click behavior
			event.preventDefault();
 
			// Store hash
			var hash = this.hash;
 
			// Using jQuery's animate() method to add smooth page scroll
			// The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
			$('html, body').animate({
				scrollTop: $(hash).offset().top
			}, 800, function(){
		
				// Add hash (#) to URL when done scrolling (default click behavior)
				window.location.hash = hash;
			});
			} // End if
		});
		});
	</script>
</head>
<body>
	<section class="app">
		<aside class="sidebar">
			<header style="font-weight: 999;" class="sidebar-navig">
				Natural Language Processing
			</header>
			<nav class="sidebar-nav">
				<ul>
					<li>
						<a href="#"><i class="ion-bag"></i> <span class="topic-decoration">&bull; Basic NLP</span></a>
						<ul class="nav-flyout">
							<li>
								<a href="#section0"><i class="ion-ios-color-filter-outline"></i>NLP pipeline</a>
							</li>
							<li>
								<a href="#section1"><i class="ion-ios-color-filter-outline"></i>Data Augmentation</a>
							</li>
							<li>
								<a href="#section2"><i class="ion-ios-clock-outline"></i>Tokenization</a>
							</li>
							<li>
								<a href="#section3"><i class="ion-ios-color-filter-outline"></i>Text Preprocessing</a>
							</li>
						 	<li>
								<a href="#section4"><i class="ion-ios-clock-outline"></i>Regular Expressions</a>
							</li>
						 	<li>
								<a href="#section5"><i class="ion-ios-color-filter-outline"></i>POS Tagging</a>
							</li>
							<li>
								<a href="#section6"><i class="ion-ios-clock-outline"></i>Stop Words</a>
							</li>
							<li>
								<a href="#section7"><i class="ion-ios-clock-outline"></i>Dependency Parsing</a>
							</li>
							<li>
								<a href="#section8"><i class="ion-ios-clock-outline"></i>NER</a>
							</li>
							<li>
								<a href="#section9"><i class="ion-ios-clock-outline"></i>Coreference Resolution</a>
							</li>
						</ul>
					</li>
					<li>
						<a href="#section10"><i class="ion-bag"></i> <span class="topic-decoration">&bull; NLTK (+BeautifulSoup, Inflect, Contractions)</span></a>
					</li>
					<li>
						<a href="#section11"><i class="ion-bag"></i> <span class="topic-decoration">&bull; SpaCy</span></a>
					</li>
					<li>
						<a href="#section12"><i class="ion-bag"></i> <span class="topic-decoration">&bull; Textblob</span></a>
					</li>
					<li>
						<a href="#section13"><i class="ion-bag"></i> <span class="topic-decoration">&bull; Word Vectors</span></a>
					</li>
				</ul>
				<ul>
					<h2></h2>
					<li>
						<a href="{{ url_for( 'index' ) }}"><i class="ion-bag"></i> <span style="color: #00FF00">&#8227 Home Page</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'about', name='pito' ) }}"><i class="ion-ios-settings"></i> <span style="color: #00FF00">&#8227 About Pito</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'about', name='bart' ) }}"><i class="ion-ios-settings"></i> <span style="color: #00FF00">&#8227 About Bart</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'projectsPage' ) }}"><i class="ion-ios-settings"></i> <span style="color: #00FF00">&#8227 Projects Page</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'learningPage' ) }}"><i class="ion-ios-settings"></i> <span style="color: #00FF00">&#8227 Learning Page</span></a>
					</li>
				</ul>
			</nav>
		</aside>
		<div id="section0">
			<br/><a class="topic" style="text-decoration: underline;">NLP pipeline</a><br/>
			<br/><br/><img img class='imgPos' src="../static/views/pipl1.png" alt="cr1" style="margin-left: 20%">
			<br/><br/><p class="basicText2">
				Coreference resolution is an optional step that isn’t always done.	
				<br/><br/>it’s worth mentioning that these are the steps in a typical NLP pipeline, but you will skip steps or re-order steps depending on what you want to do and how your NLP library is implemented. For example, some libraries like spaCy do sentence segmentation much later in the pipeline using the results of the 
				dependency parse.
			</p>
		</div>
		<div id="section1">
			<br/><a class="topic" style="text-decoration: underline;">Data Augmentation</a><br/>
			<br/><h2 class="basicText">1. Easiest Data Augmentation Techniques in Natural Language Processing:</h2>
			<p class="basicText2">
				<br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">Synonym Replacement:</span><br/> Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.
				<br/><br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">Random Insertion:</span> <br/>Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times.
				<br/><br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">Random Swap:</span> <br/>Randomly choose two words in the sentence and swap their positions. Do this n times.
				<br/><br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">Random Deletion:</span> <br/>Randomly remove each word in the sentence with probability p.
				<br/><br/><a href="https://github.com/jasonwei20/eda_nlp/blob/master/code/eda.py" style="font-weight: bold; text-decoration: underline; color: black; font-size: 1.2vw;">Code Implementation</a>
			</p>
		</div>
		<div id="section2">
			<br/><a class="topic" style="text-decoration: underline;">Tokenization</a><br/>
			<br/><h2 class="basicText">1. Description:</h2>
			<p class="basicText2">
				<br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">The process of segmenting running text into words and sentences.</span>
				Electronic text is a linear sequence of symbols (characters or words or phrases). Naturally, before any real text processing is to be done, text needs to be segmented into linguistic units such as words, punctuation, numbers, alpha-numerics, etc. This process is called tokenization.
				In English, words are often separated from each other by blanks (white space), but not all white space is equal. Both “Los Angeles” and “rock 'n' roll” are individual thoughts despite the fact that they contain multiple words and spaces. We may also need to separate single words like “I'm” into separate words “I” and “am”.
				Tokenization is a kind of pre-processing in a sense; an identification of basic units to be processed. It is conventional to concentrate on pure analysis or generation while taking basic units for granted. Yet without these basic units clearly segregated it is impossible to carry out any analysis or generation.
				The identification of units that do not need to be further decomposed for subsequent processing is an extremely important one. Errors made at this stage are very likely to induce more errors at later stages of text processing and are therefore very dangerous.
				<br/><br/><br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">More information can be found at this link:</span>
				<br/><br/><a href="https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en" style="font-weight: bold; text-decoration: underline; color: black; font-size: 1.2vw;">The Art of Tokenization</a>
			</p>
		</div>
		<div id="section3">
			<br/><a class="topic" style="text-decoration: underline;">Text Preprocessing</a><br/>

			<br/><h2 class="basicText">What is text preprocessing?</h2>
			<p class="basicText2">
				<br/>&bull; To preprocess your text simply means to bring your text into a form that is <span style="font-weight: bold; font-size: 1.0vw;">predictable</span> and <span style="font-weight: bold; font-size: 1.0vw;">analyzable</span> for your task. A task here is a combination of approach and domain. For example, extracting 
				top keywords with TF-IDF (approach) from Tweets (domain) is an example of a Task.
			</p>
			<br/><h2 class="basicText">Types of text preprocessing techniques:</h2>
			<p class="basicText2">
				<span style="font-weight: bold; font-size: 1.1vw;">&bull; Lowercasing</span>
				<br/>Lowercasing ALL your text data, although commonly overlooked, is one of the simplest and most effective form of text preprocessing. It is applicable to most text mining and NLP problems and can help in cases where your dataset is not very large and significantly helps with consistency of expected output.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Stemming</span>
				<br/>Stemming is the process of reducing inflection in words (e.g. troubled, troubles) to their root form (e.g. trouble). The “root” in this case may not be a real root word, but just a canonical form of the original word.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Lemmatization</span>
				<br/>Lemmatization on the surface is very similar to stemming, where the goal is to remove inflections and map a word to its root form. The only difference is that, lemmatization tries to do it the proper way. It doesn’t just chop things off, it actually transforms words to the actual root. For example, the word “better”
				would map to “good”. It may use a dictionary such as WordNet for mappings or some special rule-based approaches.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Stopword Removal</span>
				<br/>Stop words are a set of commonly used words in a language. Examples of stop words in English are “a”, “the”, “is”, “are” and etc. The intuition behind using stop words is that, by removing low information words from text, we can focus on the important words instead.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Normalization</span>
				<br/>A highly overlooked preprocessing step is text normalization. Text normalization is the process of transforming a text into a canonical (standard) form. For example, the word “gooood” and “gud” can be transformed to “good”, its canonical form. Another example is mapping of near identical words such as “stopwords”, 
				“stop-words” and “stop words” to just “stopwords”.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Noise Removal</span>
				<br/>Noise removal is about removing characters digits and pieces of text that can interfere with your text analysis. Noise removal is one of the most essential text preprocessing steps. It is also highly domain dependent.
				<br/><br/>For example, in Tweets, noise could be all special characters except hashtags as it signifies concepts that can characterize a Tweet. The problem with noise is that it can produce results that are inconsistent in your downstream tasks.
				<br/><br/><br/>&bull; <span style="font-weight: bold; font-size: 1.1vw;">More information can be found at this link:</span>
				<br/><br/><a href="https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html" style="font-weight: bold; text-decoration: underline; color: black; font-size: 1.2vw;">All you need to know about text preprocessing for NLP and Machine Learning</a>
			</p>
		</div>
		<div id="section4">
			<br/><a class="topic" style="text-decoration: underline;">Regular Expressions</a><br/>
			<br/><h2 class="basicText">Basic Topics:</h2>
			<p class="basicText2">
				<br/><span style="font-weight: bold; font-size: 1.2vw;">Anchors:&nbsp;&nbsp;  ^ and $  </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">^The</span>matches any string that starts with The
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">end$</span>matches a string that ends with end
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">^The end$</span>exact string match (starts and ends with The end)
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">roar</span>matches any string that has the text roar in it
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Quantifiers:&nbsp;&nbsp; * + ? and {} </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc*</span>matches a string that has ab followed by zero or more c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc+</span>matches a string that has ab followed by one or more c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc?</span>matches a string that has ab followed by zero or one c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc{2}</span>matches a string that has ab followed by 2 c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc{2,}</span>matches a string that has ab followed by 2 or more c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc{2,5}</span>matches a string that has ab followed by 2 up to 5 c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(bc)*</span>matches a string that has a followed by zero or more copies of the sequence bc
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(bc){2,5}</span>matches a string that has a followed by 2 up to 5 copies of the sequence bc
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">OR operator:&nbsp;&nbsp; | or [] </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(b|c)</span>matches a string that has a followed by b or c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a[bc]</span>same as previous
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Character Classes:&nbsp;&nbsp;  \d \w \s and .</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\d</span>matches a single character that is a digit
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\w</span>matches a word character (alphanumeric character plus underscore)
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\s</span>matches a whitespace character (includes tabs and line breaks)
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">.</span>matches any character
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\D</span>matches a single non-digit character
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\W</span>matches any non-word character (equal to [^a-zA-Z0-9_])
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\S</span>matches any non-whitespace character (equal to [^\r\n\t\f\v ])
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw; padding-right: 5em; margin-left:2%;">In order to be taken literally, you must escape the characters ^.[$()|*+?{\with a backslash \ as they have special meaning.</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\$\d:&nbsp;&nbsp;</span>matches a string that has a $ before one digit
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Flags </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">g</span>(global) does not return after the first match, restarting the subsequent searches from the end of the previous match
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">m</span>(multi-line) when enabled ^ and $ will match the start and end of a line, instead of the whole string
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">i</span>(insensitive) makes the whole expression case-insensitive (for instance /aBc/i would match AbC)
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Grouping and Capturing:&nbsp;&nbsp; () </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(bc)</span>parentheses create a capturing group with value bc
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(?:bc)*</span>using ?: we disable the capturing group
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(?<foo>bc)</span>using ?<foo> we put a name to the group
			</p>
			<p class="basicText2">
				<br/><span style="font-weight: bold; font-size: 1.2vw;">Bracket Expressions:&nbsp;&nbsp;  []</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">[abc]</span>matches a string that has either an a or a b or a c 
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">[a-c]</span>same as previous
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">[a-fA-F0-9]</span>a string that represents a single hexadecimal digit, case insensitively
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">[0-9]%</span>a string that has a character from 0 to 9 before a % sign
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">[^a-zA-Z]</span>a string that has not a letter from a to z or from A to Z. In this case the ^ is used as negation of the expression
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">Remember that inside bracket expressions all special characters (including the backslash \) lose their special powers: thus we will not apply the “escape rule”.</span>
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Greedy and Lazy match:</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;"><.+?></span> matches any character one or more times included inside < and >, expanding as needed
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;"><[^<>]+> =</span>matches any character except < or > one or more times included inside < and >
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Boundaries:&nbsp;&nbsp;  \b and \B</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\babc\b</span> performs a "whole words only"</and>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\Babc\B</span>matches only if the pattern is fully surrounded by word characters
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Back-references:&nbsp;&nbsp; \1 </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">([abc])\1</span> using \1 it matches the same text that was matched by the first capturing group
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">([abc])([de])\2\1 </span> we can use \2 (\3, \4, etc.) to identify the same text that was matched by the second (third, fourth, etc.) capturing group
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">(?<foo>[abc])\k<foo></foo></span>we put the name foo to the group and we reference it later (\k<foo>). The result is the same of the first regex
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Look-ahead and Look-behind:&nbsp;&nbsp; (?=) and (?&lt;=) </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">d(?=r)</span>matches a d only if is followed by r, but r will not be part of the overall regex match
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">(?&lt;=r)d</span>matches a d only if is preceded by an r, but r will not be part of the overall regex match
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">d(?!r)</span>matches a d only if is not followed by r, but r will not be part of the overall regex match
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">(?&lt;!r)d</span>matches a d only if is not preceded by an r, but r will not be part of the overall regex match
			</p>
		</div>
		<div id="section5">
			<br/><a class="topic" style="text-decoration: underline;">Part-Of-Speech (POS) Tagging</a><br/>
			<br/><h2 class="basicText">Generative VS Discriminative Models:</h2>
			<p class="basicText2">
				<br/><br/><img img class='imgPos' src="../static/views/gvsd.png" alt="gen vs disc model" style="margin-left: 20%">
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Generative Algorithm</span> models how the data was generated in order to categorize a signal. It asks the question: based on my generation 
				assumptions, which category is most likely to generate this signal?
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Discriminative Algorithm</span> does not care about how the data was generated, it simply categorizes a given signal.
			</p>
			<br/><h2 class="basicText">Goal:</h2>
			<p class="basicText2">
				<br/>POS tagging is the process of marking up a word in a corpus to a corresponding part of a speech tag, based on its context and definition.
			</p>
			<br/><br/><h2 class="basicText">The Different POS Tagging Techniques</h2>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Lexical Based Methods</span>  Assigns the POS tag the most frequently occurring with a word in the training corpus.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Rule-Based Methods</span> Assigns POS tags based on rules. For example, we can have a rule that says, words ending 
				with “ed” or “ing” must be assigned to a verb. Rule-Based Techniques can be used along with Lexical Based approaches to allow POS Tagging of words that are not present in the training corpus but are there in the testing data.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Probabilistic Methods</span>  This method assigns the POS tags based on the probability of a particular tag sequence occurring. Conditional Random Fields (CRFs) and Hidden Markov Models (HMMs) are probabilistic approaches to assign a POS Tag.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Deep Learning Methods</span>  Recurrent Neural Networks can also be used for POS tagging.
			</p>
		</div>
		<div id="section6">
			<br/><a class="topic" style="text-decoration: underline;">Stop Words</a><br/>
			<br/><h2 class="basicText">&bull; What are stop words?</h2>
			<p class="basicText2">
				<span style="font-weight: bold; font-size: 1.0vw;">“Stop words”</span> are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts.
			</p>
			<br/><br/><h2 class="basicText">&bull; when should I remove stop words?</h2>
			<br/><p class="basicText2">
				Problems like sentiment analysis are much more sensitive to stop words removal than document classification.
				<br/><br/>You should remove these tokens only if they don’t add any new information for your problem. Classification problems normally don’t need stop words because it’s possible to talk about the general idea of a text even if you 
				remove stop words from it.
				<br/><br/>So, for theme classification, stop words are useless. In any other case, it’s better to keep these words and do some tests with and without them so see how it affects the model. Anyway, you should <span style="font-weight: bold; font-size: 1.0vw;">never remove stop
				words without thinking about the impact of these words on the problem you are trying to solve.</span>
			</p>
		</div>
		<div id="section7">
			<br/><a class="topic" style="text-decoration: underline;">Dependency Parsing</a><br/>
			<br/><h2 class="basicText">&bull; What is Dependency parsing?</h2>
			<br/><p class="basicText2">
				Dependency parsing figure out how all the words in a sentence relate to each other.
			</p>
			<br/><h2 class="basicText">&bull; GOAL:</h2>
			<p class="basicText2">
					The goal is to build a tree that assigns a single parent word to each word in the sentence. The root of the tree will be the main verb in the sentence. 
			</p>
			<br/><br/><img img class='imgPos' src="../static/views/dp1.png" alt="dp1" style="margin-left: 20%">
			<br/><br/><p class="basicText2">
				But we can go one step further. In addition to identifying the parent word of each word, we can also predict the type of relationship that exists between those two words:
				<br/><br/><img img class='imgPos' src="../static/views/dp2.png" alt="dp2" style="margin-left: 20%">
			</p>
			<br/><br/><h2 class="basicText">&bull; Finding Noun Phrases:</h2>
			<p class="basicText2">
				<br/>So far, we’ve treated every word in our sentence as a separate entity. But sometimes it makes more sense to group together the words that represent a single idea or thing. We can use the information from the dependency parse tree to automatically group together words that are all talking about the same thing.
				<br/><br/>For example, instead of this:
				<br/><br/><img img class='imgPos' src="../static/views/dp3.png" alt="dp1" style="margin-left: 20%">
				<br/><br/>We can group the noun phrases to generate this:
				<br/><br/><img img class='imgPos' src="../static/views/dp4.png" alt="dp1" style="margin-left: 20%">
				<br/><br/>Whether or not we do this step depends on our end goal. But it’s often a quick and easy way to simplify the sentence if we don’t need extra detail about which words are adjectives and instead care more about extracting complete ideas.
			</p>
		</div>
		<div id="section8">
			<br/><a class="topic" style="text-decoration: underline;">Named Entity Recognition (NER)</a><br/>
			<br/><h2 class="basicText">&bull; Description</h2>
			<p class="basicText2">
				In our sentence, we have the following nouns:
				<br/><br/><img img class='imgPos' src="../static/views/ner1.png" alt="dp1" style="margin-left: 20%">
				<br/><br/>Some of these nouns present real things in the world. For example, “London”, “England” and “United Kingdom” represent physical places on a map. It would be nice to be able to detect that! With that information, we could automatically extract a list of real-world places mentioned in a document using NLP.
				<br/><br/>The goal of Named Entity Recognition, or NER, is to detect and label these nouns with the real-world concepts that they represent. Here’s what our sentence looks like after running each token through our NER tagging model:
				<br/><br/><img img class='imgPos' src="../static/views/ner2.png" alt="dp1" style="margin-left: 20%">
				<br/><br/>But NER systems aren’t just doing a simple dictionary lookup. Instead, they are using the context of how a word appears in the sentence and a statistical model to guess which type of noun a word represents. A good NER system can tell the difference between “Brooklyn Decker” the person and the place “Brooklyn” 
				using context clues.
				<br/><br/>Here are just some of the kinds of objects that a typical NER system can tag:
				<br/><br/><ul class="basicText2">
					<li>People’s names</li>
					<li>Company names</li>
					<li>Geographic locations (Both physical and political)</li>
					<li>Product names</li>
					<li>Dates and times</li>
					<li>Amounts of money</li>
					<li>Names of events</li>
				</ul>
			</p>
		</div>
		<div id="section9">
			<br/><a class="topic" style="text-decoration: underline;">Coreference Resolution</a><br/>
			<br/><h2 class="basicText">&bull; Description</h2>
			<p class="basicText2">
				<br/>English is full of pronouns — words like he, she, and it. These are shortcuts that we use instead of writing out names over and over in each sentence. Humans can keep track of what these words represent based on context. But our NLP model doesn’t know what pronouns mean because it only examines one sentence at a time.
				<br/><br/>Let’s look at the third sentence in our document:
				<br/><br/>“It was founded by the Romans, who named it Londinium.”
				<br/><br/>If we parse this with our NLP pipeline, we’ll know that “it” was founded by Romans. But it’s a lot more useful to know that “London” was founded by Romans.
				<br/><br/><img img class='imgPos' src="../static/views/cr1.png" alt="cr1" style="margin-left: 20%">
			</p>
		</div>
		<div id="section10">
			<br/><a class="topic" style="text-decoration: underline;">NLTK</a><br/>
			<br/><br/><img img class='imgPos' src="../static/views/nltk1.png" alt="nltk" style="margin-left: 20%">
			<img img class='imgPos' src="../static/views/nltk2.png" alt="nltk" style="margin-left: 20%">
		</div>
		<div id="section11">
			<br/><a class="topic" style="text-decoration: underline;">SpaCy</a><br/>
			<br/><br/><img img class='imgPos' src="../static/views/spacy1.png" alt="spaCy" style="margin-left: 20%">
			<img img class='imgPos' src="../static/views/spacy2.png" alt="spaCy" style="margin-left: 20%">
		</div>
		<div id="section12">
			<br/><a class="topic" style="text-decoration: underline;">Textblob</a><br/>
			<br/><br/><img img class='imgPos' src="../static/views/textblob.png" alt="textblob" style="margin-left: 20%">
		</div>
		<div id="section13">
			<br/><a class="topic" style="text-decoration: underline;">Word Representation in Natural Language Processing</a><br/>
			<br/><h2 class="basicText">&bull; Dictionary Lookup:</h2>
			<br/><p class="basicText2">
				First, take the corpus which can be collection of words, sentences or texts. Pre-process them into an intended format. One way is to use lemmatization, which is a process of converting word to its base form. For example, given words walk, walking, walks and walked, their lemma would be walk.
				<br/><br/>After that, build the lookup dictionary by creating a mapping between words and IDs i.e. each unique word in the vocabulary is assigned an ID.
				<br/><br/>Then, for each given word, return the corresponding integer representation by looking it up in the dictionary. If the word is not present in the dictionary, the integer corresponding to the Out of Vocabulary token should be returned.
				<br/><br/>By treating tokens as integers, the model might incorrectly assume the existence of natural ordering. For example, the dictionary contains entries such as 1: “airport” and 2: “plane” . The token with greater ID value might be considered as more important by the Deep learning models than the tokens 
				with less values which is a wrong assumption. Models which are trained with this type of data are prone to failure. On the contrary, data with ordinal values such as size measures 1: “small”, 2: “medium”, 3:“large” is suitable for this case. Because there is a natural ordering in the data.
			</p>
			<br/><h2 class="basicText">&bull; One-Hot Encoding</h2>
			<br/><p class="basicText2">
				The second approach of word representation is one-hot encoding. The main idea is to create a vocabulary size vector with filled zeros except one. For a single word only corresponding column is filled with the value 1 and the rest are zero valued. The encoded tokens will consist of vector with dimension
				1 × (N+ 1), where N is the size of the dictionary and the extra 1 is added to N for the Out of Vocabulary token.
				The advantage of this encoding to ordinal representation is that it does not suffer from undesirable bias. However, its immense and sparse vector representation requires large memory for computation.
				<br/><br/><img img class='imgPos' src="../static/views/onehot1.jpeg" alt="onehot">
			</p>
			<br/><h2 class="basicText">&bull; Distributional Representation</h2>
			<br/><p class="basicText2">
				The main idea behind this approach is that words typically appearing in the similar context would have a similar meaning. The idea is to store the word-context co-occurrence matrix F in which rows represent words in the vocabulary and columns represent contexts. The context could be sliding windows over
				the training sentences, or even documents. The matrix entries consist a frequency counts or tf-idf (Term Frequency-Inverse Document Frequency) scores.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">Sentence 1:</span> Boston has available flights to major US cities.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">Sentence 2:</span> Flights to Boston were cancelled due to bad weather conditions.
				<br/><br/><img img class='imgPos' src="../static/views/dpr1.png" alt="dpr1">
				<br/><br/>Since the number of context could be very large, e.g. document might contain thousand of sentences, these methods are known for being inefficient.
			</p>
			<br/><h2 class="basicText">&bull; Word Embedding</h2>
			<p class="basicText2">
				Collective term for models that learned to map a set of words or phrases in a vocabulary to vectors of numerical values.Word Embedding is really all about improving the ability of networks to learn from text data. By representing that data as lower dimensional vectors. These vectors are called Embedding.
			</p>
		</div>
	</section>
</body>
</html>
