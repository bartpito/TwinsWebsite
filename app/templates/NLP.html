<!DOCTYPE html></script>
<html>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
<head>
	<title>NLP</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link href="{{ url_for('static', filename='bootstrap/bootstrap.css') }}" rel="stylesheet" type="text/css">
	<link href="{{ url_for('static', filename='CSS/NLP.css') }}" rel="stylesheet" type="text/css">
	<script>
		$(document).ready(function(){
		// Add smooth scrolling to all links
		$("a").on('click', function(event) {
 
			// Make sure this.hash has a value before overriding default behavior
			if (this.hash !== "") {
			// Prevent default anchor click behavior
			event.preventDefault();
 
			// Store hash
			var hash = this.hash;
 
			// Using jQuery's animate() method to add smooth page scroll
			// The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
			$('html, body').animate({
				scrollTop: $(hash).offset().top
			}, 800, function(){
		
				// Add hash (#) to URL when done scrolling (default click behavior)
				window.location.hash = hash;
			});
			} // End if
		});
		});
	</script>
</head>
<body>
	<section class="app">
		<aside class="sidebar">
			<header style="font-weight: 999;" class="sidebar-navig">
				Natural Language Processing
			</header>
			<nav class="sidebar-nav">
				<ul>
					<li>
						<a href="#"><i class="ion-bag"></i> <span class="topic-decoration">&bull; Basic NLP</span></a>
						<ul class="nav-flyout">
							<li>
								<a href="#section0"><i class="ion-ios-color-filter-outline"></i>NLP pipeline</a>
							</li>
							<li>
								<a href="#section1"><i class="ion-ios-color-filter-outline"></i>Data Augmentation</a>
							</li>
							<li>
								<a href="#section2"><i class="ion-ios-clock-outline"></i>Tokenization</a>
							</li>
							<li>
								<a href="#section3"><i class="ion-ios-color-filter-outline"></i>Text Preprocessing</a>
							</li>
						 	<li>
								<a href="#section4"><i class="ion-ios-clock-outline"></i>Regular Expressions</a>
							</li>
						 	<li>
								<a href="#section5"><i class="ion-ios-color-filter-outline"></i>POS Tagging</a>
							</li>
							<li>
								<a href="#section6"><i class="ion-ios-clock-outline"></i>Stop Words</a>
							</li>
							<li>
								<a href="#section7"><i class="ion-ios-clock-outline"></i>Dependency Parsing</a>
							</li>
							<li>
								<a href="#section8"><i class="ion-ios-clock-outline"></i>NER</a>
							</li>
							<li>
								<a href="#section9"><i class="ion-ios-clock-outline"></i>Coreference Resolution</a>
							</li>
						</ul>
					</li>
					<li>
						<a href="#section10"><i class="ion-bag"></i> <span class="topic-decoration">&bull; NLTK (+BeautifulSoup, Inflect, Contractions)</span></a>
					</li>
					<li>
						<a href="#section11"><i class="ion-bag"></i> <span class="topic-decoration">&bull; SpaCy</span></a>
					</li>
					<li>
						<a href="#section12"><i class="ion-bag"></i> <span class="topic-decoration">&bull; Textblob</span></a>
					</li>
					<li>
						<a href="#section13"><i class="ion-bag"></i> <span class="topic-decoration">&bull; Word Vectors</span></a>
						<ul class="nav-flyout">
							<li>
								<a href="#section13"><i class="ion-ios-color-filter-outline"></i>Word Representation</a>
							</li>
							<li>
								<a href="#section14"><i class="ion-ios-color-filter-outline"></i>Word2Vec</a>
							</li>
							<li>
								<a href="#section15"><i class="ion-ios-color-filter-outline"></i>GloVe</a>
							</li>
							<li>
								<a href="#section16"><i class="ion-ios-color-filter-outline"></i>FastText </a>
							</li>
						</ul>
					</li>
					<li>
						<a href="#section17"><i class="ion-bag"></i> <span class="topic-decoration">&bull; Evaluation metrics</span></a>
						<ul class="nav-flyout">
							<li>
								<a href="#section17"><i class="ion-ios-color-filter-outline"></i>Natural Language Processing</a>
							</li>
							<li>
								<a href="#section18"><i class="ion-ios-color-filter-outline"></i>Natural Language Generation</a>
							</li>
						</ul>
					</li>
					<li>
						<a href="#section19"><i class="ion-bag"></i> <span class="topic-decoration">&bull; Attention</span></a>
					</li>
				</ul>
				<ul>
					<h2></h2>
					<li>
						<a href="{{ url_for( 'index' ) }}"><i class="ion-bag"></i> <span style="color: #00FF00">&#8227 Home Page</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'about', name='pito' ) }}"><i class="ion-ios-settings"></i> <span style="color: #00FF00">&#8227 About Pito</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'about', name='bart' ) }}"><i class="ion-ios-settings"></i> <span style="color: #00FF00">&#8227 About Bart</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'projectsPage' ) }}"><i class="ion-ios-settings"></i> <span style="color: #00FF00">&#8227 Projects Page</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'learningPage' ) }}"><i class="ion-ios-settings"></i> <span style="color: #00FF00">&#8227 Learning Page</span></a>
					</li>
				</ul>
			</nav>
		</aside>
		<div id="section0">
			<br/><a class="topic" style="text-decoration: underline;">NLP pipeline</a><br/>
			<br/><br/><img img class='imgPos' src="../static/views/pipl1.png" alt="cr1" style="margin-left: 20%">
			<br/><br/><p class="basicText2">
				Coreference resolution is an optional step that isn’t always done.	
				<br/><br/>it’s worth mentioning that these are the steps in a typical NLP pipeline, but you will skip steps or re-order steps depending on what you want to do and how your NLP library is implemented. For example, some libraries like spaCy do sentence segmentation much later in the pipeline using the results of the 
				dependency parse.
			</p>
		</div>
		<div id="section1">
			<br/><a class="topic" style="text-decoration: underline;">Data Augmentation</a><br/>
			<br/><h2 class="basicText">1. Easiest Data Augmentation Techniques in Natural Language Processing:</h2>
			<p class="basicText2">
				<br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">Synonym Replacement:</span><br/> Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.
				<br/><br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">Random Insertion:</span> <br/>Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times.
				<br/><br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">Random Swap:</span> <br/>Randomly choose two words in the sentence and swap their positions. Do this n times.
				<br/><br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">Random Deletion:</span> <br/>Randomly remove each word in the sentence with probability p.
				<br/><br/><a href="https://github.com/jasonwei20/eda_nlp/blob/master/code/eda.py" style="font-weight: bold; text-decoration: underline; color: black; font-size: 1.2vw;">Code Implementation</a>
			</p>
		</div>
		<div id="section2">
			<br/><a class="topic" style="text-decoration: underline;">Tokenization</a><br/>
			<br/><h2 class="basicText">1. Description:</h2>
			<p class="basicText2">
				<br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">The process of segmenting running text into words and sentences.</span>
				Electronic text is a linear sequence of symbols (characters or words or phrases). Naturally, before any real text processing is to be done, text needs to be segmented into linguistic units such as words, punctuation, numbers, alpha-numerics, etc. This process is called tokenization.
				In English, words are often separated from each other by blanks (white space), but not all white space is equal. Both “Los Angeles” and “rock 'n' roll” are individual thoughts despite the fact that they contain multiple words and spaces. We may also need to separate single words like “I'm” into separate words “I” and “am”.
				Tokenization is a kind of pre-processing in a sense; an identification of basic units to be processed. It is conventional to concentrate on pure analysis or generation while taking basic units for granted. Yet without these basic units clearly segregated it is impossible to carry out any analysis or generation.
				The identification of units that do not need to be further decomposed for subsequent processing is an extremely important one. Errors made at this stage are very likely to induce more errors at later stages of text processing and are therefore very dangerous.
				<br/><br/><br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">More information can be found at this link:</span>
				<br/><br/><a href="https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en" style="font-weight: bold; text-decoration: underline; color: black; font-size: 1.2vw;">The Art of Tokenization</a>
			</p>
		</div>
		<div id="section3">
			<br/><a class="topic" style="text-decoration: underline;">Text Preprocessing</a><br/>

			<br/><h2 class="basicText">What is text preprocessing?</h2>
			<p class="basicText2">
				<br/>&bull; To preprocess your text simply means to bring your text into a form that is <span style="font-weight: bold; font-size: 1.0vw;">predictable</span> and <span style="font-weight: bold; font-size: 1.0vw;">analyzable</span> for your task. A task here is a combination of approach and domain. For example, extracting 
				top keywords with TF-IDF (approach) from Tweets (domain) is an example of a Task.
			</p>
			<br/><h2 class="basicText">Types of text preprocessing techniques:</h2>
			<p class="basicText2">
				<span style="font-weight: bold; font-size: 1.1vw;">&bull; Lowercasing</span>
				<br/>Lowercasing ALL your text data, although commonly overlooked, is one of the simplest and most effective form of text preprocessing. It is applicable to most text mining and NLP problems and can help in cases where your dataset is not very large and significantly helps with consistency of expected output.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Stemming</span>
				<br/>Stemming is the process of reducing inflection in words (e.g. troubled, troubles) to their root form (e.g. trouble). The “root” in this case may not be a real root word, but just a canonical form of the original word.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Lemmatization</span>
				<br/>Lemmatization on the surface is very similar to stemming, where the goal is to remove inflections and map a word to its root form. The only difference is that, lemmatization tries to do it the proper way. It doesn’t just chop things off, it actually transforms words to the actual root. For example, the word “better”
				would map to “good”. It may use a dictionary such as WordNet for mappings or some special rule-based approaches.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Stopword Removal</span>
				<br/>Stop words are a set of commonly used words in a language. Examples of stop words in English are “a”, “the”, “is”, “are” and etc. The intuition behind using stop words is that, by removing low information words from text, we can focus on the important words instead.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Normalization</span>
				<br/>A highly overlooked preprocessing step is text normalization. Text normalization is the process of transforming a text into a canonical (standard) form. For example, the word “gooood” and “gud” can be transformed to “good”, its canonical form. Another example is mapping of near identical words such as “stopwords”, 
				“stop-words” and “stop words” to just “stopwords”.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Noise Removal</span>
				<br/>Noise removal is about removing characters digits and pieces of text that can interfere with your text analysis. Noise removal is one of the most essential text preprocessing steps. It is also highly domain dependent.
				<br/><br/>For example, in Tweets, noise could be all special characters except hashtags as it signifies concepts that can characterize a Tweet. The problem with noise is that it can produce results that are inconsistent in your downstream tasks.
				<br/><br/><br/>&bull; <span style="font-weight: bold; font-size: 1.1vw;">More information can be found at this link:</span>
				<br/><br/><a href="https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html" style="font-weight: bold; text-decoration: underline; color: black; font-size: 1.2vw;">All you need to know about text preprocessing for NLP and Machine Learning</a>
			</p>
		</div>
		<div id="section4">
			<br/><a class="topic" style="text-decoration: underline;">Regular Expressions</a><br/>
			<br/><h2 class="basicText">Basic Topics:</h2>
			<p class="basicText2">
				<br/><span style="font-weight: bold; font-size: 1.2vw;">Anchors:&nbsp;&nbsp;  ^ and $  </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">^The</span>matches any string that starts with The
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">end$</span>matches a string that ends with end
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">^The end$</span>exact string match (starts and ends with The end)
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">roar</span>matches any string that has the text roar in it
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Quantifiers:&nbsp;&nbsp; * + ? and {} </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc*</span>matches a string that has ab followed by zero or more c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc+</span>matches a string that has ab followed by one or more c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc?</span>matches a string that has ab followed by zero or one c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc{2}</span>matches a string that has ab followed by 2 c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc{2,}</span>matches a string that has ab followed by 2 or more c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">abc{2,5}</span>matches a string that has ab followed by 2 up to 5 c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(bc)*</span>matches a string that has a followed by zero or more copies of the sequence bc
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(bc){2,5}</span>matches a string that has a followed by 2 up to 5 copies of the sequence bc
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">OR operator:&nbsp;&nbsp; | or [] </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(b|c)</span>matches a string that has a followed by b or c
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a[bc]</span>same as previous
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Character Classes:&nbsp;&nbsp;  \d \w \s and .</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\d</span>matches a single character that is a digit
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\w</span>matches a word character (alphanumeric character plus underscore)
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\s</span>matches a whitespace character (includes tabs and line breaks)
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">.</span>matches any character
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\D</span>matches a single non-digit character
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\W</span>matches any non-word character (equal to [^a-zA-Z0-9_])
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\S</span>matches any non-whitespace character (equal to [^\r\n\t\f\v ])
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw; padding-right: 5em; margin-left:2%;">In order to be taken literally, you must escape the characters ^.[$()|*+?{\with a backslash \ as they have special meaning.</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\$\d:&nbsp;&nbsp;</span>matches a string that has a $ before one digit
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Flags </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">g</span>(global) does not return after the first match, restarting the subsequent searches from the end of the previous match
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">m</span>(multi-line) when enabled ^ and $ will match the start and end of a line, instead of the whole string
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">i</span>(insensitive) makes the whole expression case-insensitive (for instance /aBc/i would match AbC)
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Grouping and Capturing:&nbsp;&nbsp; () </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(bc)</span>parentheses create a capturing group with value bc
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(?:bc)*</span>using ?: we disable the capturing group
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">a(?<foo>bc)</span>using ?<foo> we put a name to the group
			</p>
			<p class="basicText2">
				<br/><span style="font-weight: bold; font-size: 1.2vw;">Bracket Expressions:&nbsp;&nbsp;  []</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">[abc]</span>matches a string that has either an a or a b or a c 
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">[a-c]</span>same as previous
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">[a-fA-F0-9]</span>a string that represents a single hexadecimal digit, case insensitively
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">[0-9]%</span>a string that has a character from 0 to 9 before a % sign
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">[^a-zA-Z]</span>a string that has not a letter from a to z or from A to Z. In this case the ^ is used as negation of the expression
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">Remember that inside bracket expressions all special characters (including the backslash \) lose their special powers: thus we will not apply the “escape rule”.</span>
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Greedy and Lazy match:</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;"><.+?></span> matches any character one or more times included inside < and >, expanding as needed
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;"><[^<>]+> =</span>matches any character except < or > one or more times included inside < and >
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Boundaries:&nbsp;&nbsp;  \b and \B</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\babc\b</span> performs a "whole words only"</and>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">\Babc\B</span>matches only if the pattern is fully surrounded by word characters
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Back-references:&nbsp;&nbsp; \1 </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">([abc])\1</span> using \1 it matches the same text that was matched by the first capturing group
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">([abc])([de])\2\1 </span> we can use \2 (\3, \4, etc.) to identify the same text that was matched by the second (third, fourth, etc.) capturing group
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">(?<foo>[abc])\k<foo></foo></span>we put the name foo to the group and we reference it later (\k<foo>). The result is the same of the first regex
			</p>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Look-ahead and Look-behind:&nbsp;&nbsp; (?=) and (?&lt;=) </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">d(?=r)</span>matches a d only if is followed by r, but r will not be part of the overall regex match
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">(?&lt;=r)d</span>matches a d only if is preceded by an r, but r will not be part of the overall regex match
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">d(?!r)</span>matches a d only if is not followed by r, but r will not be part of the overall regex match
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw; padding-right: 5em; margin-left:2%;">(?&lt;!r)d</span>matches a d only if is not preceded by an r, but r will not be part of the overall regex match
			</p>
		</div>
		<div id="section5">
			<br/><a class="topic" style="text-decoration: underline;">Part-Of-Speech (POS) Tagging</a><br/>
			<br/><h2 class="basicText">Generative VS Discriminative Models:</h2>
			<p class="basicText2">
				<br/><br/><img img class='imgPos' src="../static/views/gvsd.png" alt="gen vs disc model" style="margin-left: 20%">
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Generative Algorithm</span> models how the data was generated in order to categorize a signal. It asks the question: based on my generation 
				assumptions, which category is most likely to generate this signal?
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Discriminative Algorithm</span> does not care about how the data was generated, it simply categorizes a given signal.
			</p>
			<br/><h2 class="basicText">Goal:</h2>
			<p class="basicText2">
				<br/>POS tagging is the process of marking up a word in a corpus to a corresponding part of a speech tag, based on its context and definition.
			</p>
			<br/><br/><h2 class="basicText">The Different POS Tagging Techniques</h2>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Lexical Based Methods</span>  Assigns the POS tag the most frequently occurring with a word in the training corpus.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Rule-Based Methods</span> Assigns POS tags based on rules. For example, we can have a rule that says, words ending 
				with “ed” or “ing” must be assigned to a verb. Rule-Based Techniques can be used along with Lexical Based approaches to allow POS Tagging of words that are not present in the training corpus but are there in the testing data.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Probabilistic Methods</span>  This method assigns the POS tags based on the probability of a particular tag sequence occurring. Conditional Random Fields (CRFs) and Hidden Markov Models (HMMs) are probabilistic approaches to assign a POS Tag.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Deep Learning Methods</span>  Recurrent Neural Networks can also be used for POS tagging.
			</p>
		</div>
		<div id="section6">
			<br/><a class="topic" style="text-decoration: underline;">Stop Words</a><br/>
			<br/><h2 class="basicText">&bull; What are stop words?</h2>
			<p class="basicText2">
				<span style="font-weight: bold; font-size: 1.0vw;">“Stop words”</span> are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts.
			</p>
			<br/><br/><h2 class="basicText">&bull; when should I remove stop words?</h2>
			<br/><p class="basicText2">
				Problems like sentiment analysis are much more sensitive to stop words removal than document classification.
				<br/><br/>You should remove these tokens only if they don’t add any new information for your problem. Classification problems normally don’t need stop words because it’s possible to talk about the general idea of a text even if you 
				remove stop words from it.
				<br/><br/>So, for theme classification, stop words are useless. In any other case, it’s better to keep these words and do some tests with and without them so see how it affects the model. Anyway, you should <span style="font-weight: bold; font-size: 1.0vw;">never remove stop
				words without thinking about the impact of these words on the problem you are trying to solve.</span>
			</p>
		</div>
		<div id="section7">
			<br/><a class="topic" style="text-decoration: underline;">Dependency Parsing</a><br/>
			<br/><h2 class="basicText">&bull; What is Dependency parsing?</h2>
			<br/><p class="basicText2">
				Dependency parsing figure out how all the words in a sentence relate to each other.
			</p>
			<br/><h2 class="basicText">&bull; GOAL:</h2>
			<p class="basicText2">
					The goal is to build a tree that assigns a single parent word to each word in the sentence. The root of the tree will be the main verb in the sentence. 
			</p>
			<br/><br/><img img class='imgPos' src="../static/views/dp1.png" alt="dp1" style="margin-left: 20%">
			<br/><br/><p class="basicText2">
				But we can go one step further. In addition to identifying the parent word of each word, we can also predict the type of relationship that exists between those two words:
				<br/><br/><img img class='imgPos' src="../static/views/dp2.png" alt="dp2" style="margin-left: 20%">
			</p>
			<br/><br/><h2 class="basicText">&bull; Finding Noun Phrases:</h2>
			<p class="basicText2">
				<br/>So far, we’ve treated every word in our sentence as a separate entity. But sometimes it makes more sense to group together the words that represent a single idea or thing. We can use the information from the dependency parse tree to automatically group together words that are all talking about the same thing.
				<br/><br/>For example, instead of this:
				<br/><br/><img img class='imgPos' src="../static/views/dp3.png" alt="dp1" style="margin-left: 20%">
				<br/><br/>We can group the noun phrases to generate this:
				<br/><br/><img img class='imgPos' src="../static/views/dp4.png" alt="dp1" style="margin-left: 20%">
				<br/><br/>Whether or not we do this step depends on our end goal. But it’s often a quick and easy way to simplify the sentence if we don’t need extra detail about which words are adjectives and instead care more about extracting complete ideas.
			</p>
		</div>
		<div id="section8">
			<br/><a class="topic" style="text-decoration: underline;">Named Entity Recognition (NER)</a><br/>
			<br/><h2 class="basicText">&bull; Description</h2>
			<p class="basicText2">
				In our sentence, we have the following nouns:
				<br/><br/><img img class='imgPos' src="../static/views/ner1.png" alt="dp1" style="margin-left: 20%">
				<br/><br/>Some of these nouns present real things in the world. For example, “London”, “England” and “United Kingdom” represent physical places on a map. It would be nice to be able to detect that! With that information, we could automatically extract a list of real-world places mentioned in a document using NLP.
				<br/><br/>The goal of Named Entity Recognition, or NER, is to detect and label these nouns with the real-world concepts that they represent. Here’s what our sentence looks like after running each token through our NER tagging model:
				<br/><br/><img img class='imgPos' src="../static/views/ner2.png" alt="dp1" style="margin-left: 20%">
				<br/><br/>But NER systems aren’t just doing a simple dictionary lookup. Instead, they are using the context of how a word appears in the sentence and a statistical model to guess which type of noun a word represents. A good NER system can tell the difference between “Brooklyn Decker” the person and the place “Brooklyn” 
				using context clues.
				<br/><br/>Here are just some of the kinds of objects that a typical NER system can tag:
				<br/><br/><ul class="basicText2">
					<li>People’s names</li>
					<li>Company names</li>
					<li>Geographic locations (Both physical and political)</li>
					<li>Product names</li>
					<li>Dates and times</li>
					<li>Amounts of money</li>
					<li>Names of events</li>
				</ul>
			</p>
		</div>
		<div id="section9">
			<br/><a class="topic" style="text-decoration: underline;">Coreference Resolution</a><br/>
			<br/><h2 class="basicText">&bull; Description</h2>
			<p class="basicText2">
				<br/>English is full of pronouns — words like he, she, and it. These are shortcuts that we use instead of writing out names over and over in each sentence. Humans can keep track of what these words represent based on context. But our NLP model doesn’t know what pronouns mean because it only examines one sentence at a time.
				<br/><br/>Let’s look at the third sentence in our document:
				<br/><br/>“It was founded by the Romans, who named it Londinium.”
				<br/><br/>If we parse this with our NLP pipeline, we’ll know that “it” was founded by Romans. But it’s a lot more useful to know that “London” was founded by Romans.
				<br/><br/><img img class='imgPos' src="../static/views/cr1.png" alt="cr1" style="margin-left: 20%">
			</p>
		</div>
		<div id="section10">
			<br/><a class="topic" style="text-decoration: underline;">NLTK</a><br/>
			<br/><br/><img img class='imgPos' src="../static/views/nltk1.png" alt="nltk" style="margin-left: 20%">
			<img img class='imgPos' src="../static/views/nltk2.png" alt="nltk" style="margin-left: 20%">
		</div>
		<div id="section11">
			<br/><a class="topic" style="text-decoration: underline;">SpaCy</a><br/>
			<br/><br/><img img class='imgPos' src="../static/views/spacy1.png" alt="spaCy" style="margin-left: 20%">
			<img img class='imgPos' src="../static/views/spacy2.png" alt="spaCy" style="margin-left: 20%">
		</div>
		<div id="section12">
			<br/><a class="topic" style="text-decoration: underline;">Textblob</a><br/>
			<br/><br/><img img class='imgPos' src="../static/views/textblob.png" alt="textblob" style="margin-left: 20%">
		</div>
		<div id="section13">
			<br/><a class="topic" style="text-decoration: underline;">Word Representation in Natural Language Processing</a><br/>
			<br/><h2 class="basicText">&bull; Dictionary Lookup:</h2>
			<br/><p class="basicText2">
				First, take the corpus which can be collection of words, sentences or texts. Pre-process them into an intended format. One way is to use lemmatization, which is a process of converting word to its base form. For example, given words walk, walking, walks and walked, their lemma would be walk.
				<br/><br/>After that, build the lookup dictionary by creating a mapping between words and IDs i.e. each unique word in the vocabulary is assigned an ID.
				<br/><br/>Then, for each given word, return the corresponding integer representation by looking it up in the dictionary. If the word is not present in the dictionary, the integer corresponding to the Out of Vocabulary token should be returned.
				<br/><br/>By treating tokens as integers, the model might incorrectly assume the existence of natural ordering. For example, the dictionary contains entries such as 1: “airport” and 2: “plane” . The token with greater ID value might be considered as more important by the Deep learning models than the tokens 
				with less values which is a wrong assumption. Models which are trained with this type of data are prone to failure. On the contrary, data with ordinal values such as size measures 1: “small”, 2: “medium”, 3:“large” is suitable for this case. Because there is a natural ordering in the data.
			</p>
			<br/><h2 class="basicText">&bull; One-Hot Encoding</h2>
			<br/><p class="basicText2">
				The second approach of word representation is one-hot encoding. The main idea is to create a vocabulary size vector with filled zeros except one. For a single word only corresponding column is filled with the value 1 and the rest are zero valued. The encoded tokens will consist of vector with dimension
				1 × (N+ 1), where N is the size of the dictionary and the extra 1 is added to N for the Out of Vocabulary token.
				The advantage of this encoding to ordinal representation is that it does not suffer from undesirable bias. However, its immense and sparse vector representation requires large memory for computation.
				<br/><br/><img img class='imgPos' src="../static/views/onehot1.jpeg" alt="onehot">
			</p>
			<br/><h2 class="basicText">&bull; Distributional Representation</h2>
			<br/><p class="basicText2">
				The main idea behind this approach is that words typically appearing in the similar context would have a similar meaning. The idea is to store the word-context co-occurrence matrix F in which rows represent words in the vocabulary and columns represent contexts. The context could be sliding windows over
				the training sentences, or even documents. The matrix entries consist a frequency counts or tf-idf (Term Frequency-Inverse Document Frequency) scores.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">Sentence 1:</span> Boston has available flights to major US cities.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">Sentence 2:</span> Flights to Boston were cancelled due to bad weather conditions.
				<br/><br/><img img class='imgPos' src="../static/views/dpr1.png" alt="dpr1">
				<br/><br/>Since the number of context could be very large, e.g. document might contain thousand of sentences, these methods are known for being inefficient.
			</p>
			<br/><h2 class="basicText">&bull;TF-IDF</h2></p>
			<p class="basicText2">
					<br/>In NLP projects, we are required to determine the importance of each word. TF-IDF is a great statistical measure. It helps us understand the relevance of the term (word).
					<br/><br/>For each term in a document, a matrix is computed by performing following 3 steps:
					<br/><br/>1. Calculate the frequency of a term in a document. This is known as Term Frequency (TF). This is achieved by dividing the number of times a term appears in a document divided by the total number of terms in a document.
					<br/><br/>2. Calculate the inverse of document frequency of a term. This is computed by dividing the total number of documents by the number of documents that contain the term. The inverse is calculated so that we can compute a positive log value. Therefore, compute a log of the computed value. This will result in a positive value. This is known as Inverse Document Frequency (IDF).
					<br/><br/>3. Finally multiply step 1 by step 3. This is known as TF-IDF
					<br/><br/>Rows of the matrix represent the terms and the columns of the matrix are the document names.
			</p>
			<br/><h2 class="basicText">&bull; Word Embedding</h2>
			<p class="basicText2">
				<br/>Collective term for models that learned to map a set of words or phrases in a vocabulary to vectors of numerical values.Word Embedding is really all about improving the ability of networks to learn from text data. By representing that data as lower dimensional vectors. These vectors are called Embedding.
			</p>
			<br/><h2 class="basicText">&bull; How it is done?</h2>
			<br/><p class="basicText2">
				General approach for dealing with words in your text data is to one-hot encode your text. You will have tens of thousands of unique words in your text vocabulary. Computations with such one-hot encoded vectors for these words will be very inefficient because most values in your one-hot vector will be 0. 
				So, the matrix calculation that will happen in between a one-hot vector and a first hidden layer will result in a output that will have mostly 0 values
				<br/><br/><img img class='imgPos' src="../static/views/wv2.png" alt="wv">
				<br/><br/>Now, instead of doing the matrix multiplication between the inputs and hidden layer we directly grab the values from embedding weight matrix. We can do this because the multiplication of one-hot vector with weight matrix returns the row of the matrix corresponding to the index of ‘1’ input unit
				<br/><br/><img img class='imgPos' src="../static/views/wv3.png" alt="wv">
				<br/><br/>So, we use this Weight Matrix as lookup table. We encode the words as integers, for example ‘cool’ is encoded as 512, ‘hot’ is encoded as 764. Then to get hidden layer output value for ‘cool’ we just simply need to lookup the 512th row in the weight matrix. This process is called Embedding Lookup. 
				The number of dimension in the hidden layer output is the embedding dimension
				<br/><br/><img img class='imgPos' src="../static/views/wv4.png" alt="wv">
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Summary:</span>
				<br/><br/>&bull; The embedding layer is just a hidden layer
				<br/><br/>&bull; The lookup table is just a embedding weight matrix
				<br/><br/>&bull; The lookup is just a shortcut for matrix multiplication
				<br/><br/>&bull; The lookup table is trained just like any weight matrix
			</p>
		</div>
		<div id="section14">
			<br/><a class="topic" style="text-decoration: underline;">Word2Vec</a><br/>
			<br/><p class="basicText2">
				<span style="font-weight: bold; font-size: 1.0vw;">Introduction</span>
				<br/>Word2Vec model is used for learning vector representations of words called “word embeddings”. This is typically done as a preprocessing step, after which the learned vectors are fed into a discriminative model (typically an RNN) to generate predictions and perform all sort of interesting things.
				<br/><br/><br/><img img class='imgPos' src="../static/views/dvg1.png" alt="dvg">
				<br/><br/><br/><br/><span style="font-weight: bold; font-size: 1.0vw;">Co-occurrence matrix</span>
				<br/><br/>A co-occurrence matrix is a matrix that contains the number of counts of each word appearing next to all the other words in the corpus (or training set).
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">"I love NLP and I like dogs"</span>
				<br/><br/><br/><img img class='imgPos' src="../static/views/com.png" alt="co-occurrence">
				<br/><br/><br/><img img class='imgPos' src="../static/views/wv5.png" alt="wv">
				<br/><br/>Notice that through this simple matrix, we’re able to gain pretty useful insights. For example, notice that the words ‘love’ and ‘like’ both contain 1’s for their counts with nouns (NLP and dogs). They also have 1’s for the count with “I”, thus indicating that the words must be some sort of verb. 
				With a larger dataset than just one sentence, you can imagine that this similarity will become more clear as ‘like’, ‘love’, and other synonyms will begin to have similar word vectors, because of the fact that they are used in similar contexts.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">Now, although this a great starting point, we notice that the dimensionality of each word will increase linearly with the size of the corpus.</span> If we had a million words (not really a lot in NLP standards), we’d have a million by million sized matrix which would be extremely sparse (lots of 0’s). Definitely 
				not the best in terms of storage efficiency. There have been numerous advancements in finding the most optimal ways to represent these word vectors. The most famous of which is Word2Vec.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">How does Word2Vec work?</span>
				<br/>The algorithm exists in two flavors <span style="font-weight: bold; font-size: 1.0vw;">CBOW and Skip-Gram.</span> Given a set of sentences (also called corpus) the model loops on the words of each sentence and either tries to use the current word of to predict its neighbors (its context), in which case the method is called “Skip-Gram”, or it uses each of these contexts 
				to predict the current word, in which case the￼ method is called “Continuous Bag Of Words” (CBOW). The limit on the number of words in each context is determined by a parameter called “window size”.
				<br/><br/><span style="font-weight: bold; font-size: 1.3vw;">Skip-gram model:</span>
				<br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Intuition</span>
				<br/><br/>The skip-gram neural network model is actually surprisingly simple in its most basic form. Train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.
				<br/><br/>We’re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.
				<br/><br/>The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if you gave the trained network the input word “Soviet”, the output probabilities are going to be much higher for words like “Union” and “Russia” than for unrelated words like “watermelon” and “kangaroo”.
				<br/><br/>We’ll train the neural network to do this by feeding it word pairs found in our training documents. The below example shows some of the training samples (word pairs) we would take from the sentence “The quick brown fox jumps over the lazy dog.” I’ve used a small window size of 2 just for the example. The word highlighted in blue is the input word.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">When training this network on word pairs, the input is a one-hot vector representing the input word and the training output is also a one-hot vector representing the output word. But when you evaluate the trained network on an input word, the output vector will actually be a probability distribution (i.e., a bunch of floating point values, not a one-hot vector).</span>
				<br/><br/><br/><img img class='imgPos' src="../static/views/sg1.png" alt="sg">
				<br/><br/>We’re going to represent an input word like “ants” as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) and we’ll place a “1” in the position corresponding to the word “ants”, and 0s in all of the other positions.
				<br/><br/>The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.
				<br/><br/><br/><img img class='imgPos' src="../static/views/sg2.png" alt="sg">
				<br/><br/>There is no activation function on the hidden layer neurons, but the output neurons use softmax.
				<br/><br/>For our example, we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).
				<br/><br/>300 features is what Google used in their published model trained on the Google news dataset (you can download it from here). The number of features is a “hyper parameter” that you would just have to tune to your application (that is, try different values and see what yields the best results).
				<br/><br/>If you look at the rows of this weight matrix, these are actually what will be our word vectors!
				<br/><br/><br/><img img class='imgPos' src="../static/views/sg3.png" alt="sg">
				<br/><br/><br/><span style="font-weight: bold; font-size: 1.0vw;">&bull; Improvement</span>
				<br/><br/>We need few additional modifications to the basic skip-gram model which are important for actually making it feasible to train. Running gradient descent on a neural network that large is going to be slow. And to make matters worse, you need a huge amount of training data in order to tune that many weights and avoid over-fitting. millions of weights times billions of training samples means that training this model is going to be a beast. The authors of Word2Vec addressed these issues in their second paper.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">1. Treating common word pairs or phrases as single “words” in their model.</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">2. Subsampling frequent words to decrease the number of training examples.</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">3. Modifying the optimization objective with a technique they called “Negative Sampling”, which causes each training sample to update only a small percentage of the model’s weights.</span>
				<br/><br/>It’s worth noting that subsampling frequent words and applying Negative Sampling not only reduced the compute burden of the training process, but also improved the quality of their resulting word vectors as well.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">Subsampling: </span>
				<br/><br/>There are two “problems” with common words like “the”:
				<br/><br/>When looking at word pairs, (“fox”, “the”) doesn’t tell us much about the meaning of “fox”. “the” appears in the context of pretty much every word.
				<br/><br/>We will have many more samples of (“the”, …) than we need to learn a good vector for “the”.
				<br/><br/>Word2Vec implements a “subsampling” scheme to address this. For each word we encounter in our training text, there is a chance that we will effectively delete it from the text. The probability that we cut the word is related to the word’s frequency.
				<br/><br/>If we have a window size of 10, and we remove a specific instance of “the” from our text:
				<br/><br/>As we train on the remaining words, “the” will not appear in any of their context windows.
				<br/><br/>We’ll have 10 fewer training samples where “the” is the input word.
				<br/><br/>Note how these two effects help address the two problems stated above.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">Negative Sampling:</span>
				<br/><br/>As we discussed above, the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples!
				<br/><br/>Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Here’s how it works.
				<br/><br/>When training the network on the word pair (“fox”, “quick”), recall that the “label” or “correct output” of the network is a one-hot vector. That is, for the output neuron corresponding to “quick” to output a 1, and for all of the other thousands of output neurons to output a 0.
				<br/><br/>With negative sampling, we are instead going to randomly select just a small number of “negative” words (let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).
				<br/><br/>The paper says that selecting 5–20 words works well for smaller datasets, and you can get away with only 2–5 words for large datasets.
				<br/><br/>Recall that the output layer of our model has a weight matrix that’s 300 x 10,000. So we will just be updating the weights for our positive word (“quick”), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!
				<br/><br/>In the hidden layer, only the weights for the input word are updated (this is true whether you’re using Negative Sampling or not).
				<br/><br/>The “negative samples” (that is, the 5 output words that we’ll train to output 0) are chosen using a “unigram distribution”.
				<br/><br/>Essentially, the probability for selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples.
				<br/><br/><span style="font-weight: bold; font-size: 1.3vw;">The Continuous Bag of Words (CBOW) Model:</span>
				<br/>The CBOW model architecture tries to predict the current target word (the center word) based on the source context words (surrounding words). Considering a simple sentence, “the quick brown fox jumps over the lazy dog”, this can be pairs of (context_window, target_word) where if we consider a context window of size 2, we have examples like ([quick, fox], 
				brown), ([the, brown], quick), ([the, dog], lazy) and so on. Thus the model tries to predict the target_word based on the context_window words.
				<br/><br/><br/><img img class='imgPos' src="../static/views/cbow2.png" alt="cbow">
				<br/><br/>The input or the context word is a one hot encoded vector of size V. The hidden layer contains N neurons and the output is again a V length vector with the elements being the softmax values.
				<br/><br/>Let’s get the terms in the picture right:
				<br/><br/>- Wvn is the weight matrix that maps the input x to the hidden layer (V*N dimensional matrix)
				<br/><br/>-W`nv is the weight matrix that maps the hidden layer outputs to the final output layer (N*V dimensional matrix)
				<br/><br/>The hidden layer neurons just copy the weighted sum of inputs to the next layer. There is no activation like sigmoid, tanh or ReLU. The only non-linearity is the softmax calculations in the output layer.
				<br/><br/>But, the above model used a single context word to predict the target. We can use multiple context words to do the same.
				<br/><br/><br/><img img class='imgPos' src="../static/views/cbow1.png" alt="cbow">
				<br/><br/>The above model takes C context words. When Wvn is used to calculate hidden layer inputs, we take an average over all these C context word inputs.
			</p>
		</div>
		<div id="section15">
			<br/><a class="topic" style="text-decoration: underline;">GloVe</a><br/>
			<br/><p class="basicText2">
				<span style="font-weight: bold; font-size: 1.0vw;">Introduction</span>
				<br/>The GloVe model stands for Global Vectors which is an unsupervised learning model which can be used to obtain dense word vectors similar to Word2Vec. However the technique is different and training is performed on an aggregated global word-word co-occurrence matrix, giving us a vector space with meaningful sub-structures.
				<br/><br/><span style="font-weight: bold; font-size: 1.0vw;">The basic methodology</span>
				<br/>The basic methodology of the GloVe model is to first create a huge word-context co-occurence matrix consisting of (word, context) pairs such that each element in this matrix represents how often a word occurs with the context (which can be a sequence of words). The idea then is to apply matrix factorization to approximate this matrix as depicted in the following figure.
				<br/><br/><br/><img img class='imgPos' src="../static/views/gv1.png" alt="glove">
				<br/><br/><br/> Considering the <span style="font-weight: bold; font-size: 1.0vw;">Word-Context (WC)</span> matrix, <span style="font-weight: bold; font-size: 1.0vw;">Word-Feature (WF) </span> matrix and <span style="font-weight: bold; font-size: 1.0vw;">Feature-Context (FC)</span> matrix, we try to factorize <span style="font-weight: bold; font-size: 1.0vw;">WC = WF x FC</span>
				, such that we we aim to reconstruct <span style="font-weight: bold; font-size: 1.0vw;">WC</span> from <span style="font-weight: bold; font-size: 1.0vw;">WF</span> and <span style="font-weight: bold; font-size: 1.0vw;">FC</span> by multiplying them. For this, we typically initialize <span style="font-weight: bold; font-size: 1.0vw;">WF</span> and 
				<span style="font-weight: bold; font-size: 1.0vw;">FC </span>  with some random weights and attempt to multiply them to get <span style="font-weight: bold; font-size: 1.0vw;">WC'</span> (an approximation of WC) and measure how close it is to <span style="font-weight: bold; font-size: 1.0vw;">WC</span>. We do this multiple times using Stochastic Gradient Descent (SGD) to minimize the error.
				Finally, <span style="font-weight: bold; font-size: 1.0vw;">the Word-Feature matrix (WF) </span> gives us the word embeddings for each word where F can be preset to a specific number of dimensions. A very important point to remember is that both Word2Vec and GloVe models are very similar in how they work. Both of them aim to build a vector space where the position of each word is influenced by its neighboring words based on their context and semantics. 
				Word2Vec starts with local individual examples of word co-occurrence pairs and GloVe starts with global aggregated co-occurrence statistics across all words in the corpus.
			</p>
		</div>
		<div id="section16">
			<br/><a class="topic" style="text-decoration: underline;">FastText</a><br/>
			<br/><p class="basicText2">
				<span style="font-weight: bold; font-size: 1.2vw;">Introduction</span>
				<br/><br/>FastText is a framework for learning word representations and also performing robust, fast and accurate text classification. The framework is open-sourced by Facebook on GitHub and claims to have the following.
				<br/><br/>&bull; Recent state-of-the-art English word vectors.
				<br/><br/>&bull; Word vectors for 157 languages trained on Wikipedia and Crawl.
				<br/><br/>&bull; Models for language identification and various supervised tasks.
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">How the model works?</span>
				<br/><br/>In general, predictive models like the Word2Vec model typically considers each word as a distinct entity <span style="font-weight: bold; font-size: 1.0vw;">(e.g. where)</span> and generates a dense embedding for the word. However this poses to be a serious limitation with languages having massive vocabularies and many rare words which may not occur a lot in different corpora. The Word2Vec model typically ignores the morphological structure of each word and considers a word as a single entity. The FastText model considers each word as a Bag of Character n-grams. This is also called as a subword model in the paper.
				<br/><br/>We add special boundary symbols < and > at the beginning and end of words. This enables us to distinguish prefixes and suffixes from other character sequences. We also include the word w itself in the set of its n-grams, to learn a representation for each word (in addition to its character n-grams). Taking the word <span style="font-weight: bold; font-size: 1.0vw;">where</span> and <span style="font-weight: bold; font-size: 1.0vw;">n=3</span> (tri-grams) as an example, it will be represented by the character n-grams: <span style="font-weight: bold; font-size: 1.0vw;">< wh, whe, her, ere, re > </span> and the special sequence
				<span style="font-weight: bold; font-size: 1.0vw;">< where > </span>representing the whole word. Note that the sequence , corresponding to the word <span style="font-weight: bold; font-size: 1.0vw;">< her > </span>is different from the tri-gram <span style="font-weight: bold; font-size: 1.0vw;">her</span> from the word <span style="font-weight: bold; font-size: 1.0vw;">where.</span>
				<br/><br/>In practice, the paper recommends in extracting all the n-grams for n ≥ 3 and n ≤ 6. This is a very simple approach, and different sets of n-grams could be considered, for example taking all prefixes and suffixes. We typically associate a vector representation (embedding) to each n-gram for a word. Thus, we can represent a word by the sum of the vector representations of its n-grams or the average of the embedding of these n-grams. Thus, due to this effect of leveraging n-grams from individual words based on their characters, there is a higher chance for rare words to get a good representation since their character based n-grams
				should occur across other words of the corpus.
			</p>
		</div>
		<div id="section17">
			<a class="topic" style="text-decoration: underline;">Evaluation metrics</a><br/>
			<p class="basicText2">
				<br/><br/><span style="font-weight: bold; font-size: 1.3vw;">Natural Language Processing: </span>
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">1. Accuracy</span>
				<br/>It’s the ratio of the correctly labeled subjects to the whole pool of subjects.
				<br/>Accuracy is the most intuitive one.
				<br/>Accuracy answers the following question: How many students did we correctly label out of all the students? 
				<br/><span style="font-weight: bold; font-size: 1.0vw;">Accuracy = (TP+TN)/(TP+FP+FN+TN)</span>
				<br/><span style="font-weight: bold; font-size: 1.0vw;">Numerator:</span> all correctly labeled subject (All trues)
				<br/><span style="font-weight: bold; font-size: 1.0vw;">Denominator:</span> all subjects
				<br/><br/><img img class='imgPos' src="../static/views/cm1.jpeg" alt="confusion matrix">
				<br/><br/><br/><span style="font-weight: bold; font-size: 1.2vw;">2. Precision and Recall</span>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">Precision:</span>
				<br/><img img class='imgPos' src="../static/views/prec1.png" alt="precision">
				<br/><br/>The denominator is actually the Total Predicted Positive!
				<br/><br/><br/><img img class='imgPos' src="../static/views/prec2.png" alt="precision">
				<br/><br/><br/><img img class='imgPos' src="../static/views/prec3.png" alt="precision">
				<br/><br/>Immediately, you can see that Precision talks about how precise/accurate your model is out of those predicted positive, how many of them are actual positive.
				<br/><br/>Precision is a good measure to determine, when the costs of False Positive is high. For instance, email spam detection. In email spam detection, a false positive means that an email that is non-spam (actual negative) has been identified as spam (predicted spam). The email user might lose important emails if the precision is not high for the spam detection model.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">Recall:</span>
				<br/><br/><br/><img img class='imgPos' src="../static/views/rec1.png" alt="recall">
				<br/><br/><img img class='imgPos' src="../static/views/rec2.jpeg" alt="recall">
				<br/><br/><br/><span style="font-weight: bold; font-size: 1.2vw;">3. F1 Score</span>
				<br/><br/><img img class='imgPos' src="../static/views/f11.png" alt="f1 score">
				<br/><br/>F1 Score is needed when you want to seek a balance between Precision and Recall. So what is the difference between F1 Score and Accuracy then? We have previously seen that accuracy can be largely contributed by a large number of True Negatives which in most business circumstances, we do not focus on much whereas False Negative and False Positive usually has business costs (tangible & intangible) thus F1 Score might be a better measure to use if we need to seek a balance between Precision and Recall AND there is an uneven class distribution (large number of Actual Negatives).
				<br/><br/><br/><span style="font-weight: bold; font-size: 1.2vw;">4. Perplexity</span>
				<br/>If a machine is trying to model text (for example, a so-called language model problem), one may look at metrics identifying how well does the model match actual text - via metrics such as likelihood or Perplexity
				<br/><br/><img img class='imgPos' src="../static/views/perp1.png" alt="Perplexity" width='60%' height="60%">
			</p>
		</div>
		<div id="section18">
			<a class="topic" style="text-decoration: underline;">Natural language Generation</a><br/>
			<p class="basicText2">
				<br/><span style="font-weight: bold; font-size: 1.2vw;">1. BLEU (Bilingual Evaluation Understudy)</span>
				<br/>his is by far the most popular metric for evaluating machine translation system. In BLEU, precision and recall are approximated by modified n-gram precision and best match length, respectively.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">Modified n-gram precision:</span>
				<br/>First, an n-gram precision is the fraction of n-grams in the candidate text which are present in any of the reference texts. From the example above, the unigram precision of A is 100%. However, just using this value presents a problem. For example, consider the two candidates:
				<br/><br/>(i) He works on machine learning.
				<br/><br/>(ii) He works on on machine machine learning learning.
				<br/><br/>Candidate (i) has a unigram precision of 60% while for (ii) it is 75%. However, it is obvious that (ii) is not a better candidate than (i) in any way. To solve this problem, we use a “modified” n-gram precision. It matches the candidate’s n-grams only as many times as they are present in any of the reference texts. So in the above example, (ii)’s unigrams ‘on’, ‘machine’, and ‘learning’ are matched only once, and the unigram precision becomes 37.5%.
				<br/><br/>Finally, to include all the n-gram precision scores in our final precision, we take their geometric mean. This is done because it has been found that precision decreases exponentially with n, and as such, we would require logarithmic averaging to represent all values fairly.
				<br/><br/><img img class='imgPos' src="../static/views/bleu1.gif" alt="bleu">
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">Best match length:</span>
				<br/>While precision calculation was relatively simple, the problem with recall is that there may be many reference texts. So it is difficult to calculate the sensitivity of the candidate with respect to a general reference. However, it is intuitive to think that a longer candidate text is more likely to contain a larger fraction of some reference than a shorter candidate. At the same time, we have already ensured that candidate texts are not arbitrarily long, since then their precision score would be low.
				<br/><br/>Therefore, we can introduce recall by just penalizing brevity in candidate texts. This is done by adding a multiplicative factor BP with the modified n-gram precision as follows.
				<br/><br/><img img class='imgPos' src="../static/views/bleu2.gif" alt="bleu">
				<br/><br/>Here, c is the total length of candidate translation corpus, and r is the effective reference length of corpus, i.e., average length of all references. The lengths are taken as average over the entire corpus to avoid harshly punishing the length deviations on short sentences. As the candidate length decreases, the ratio r/c increases, and the BP decreases exponentially.
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">2. ROUGE (Recall Oriented Understudy for Gisting Evaluation)</span>
				<br/>As is clear from its name, ROUGE is based only on recall, and is mostly used for summary evaluation. Depending on the feature used for calculating recall, ROUGE may be of many types, namely ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S. Here, we describe the idea behind one of these, and then give a quick run-down of the others.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">ROUGE-N: </span>
				<br/>This is based on n-grams. For example, ROUGE-1 counts recall based on matching unigrams, and so on. For any n, we count the total number of n-grams across all the reference summaries, and find out how many of them are present in the candidate summary. This fraction is the required metric value.
				<br/><br/>ROUGE-L/W/S are based on: longest common subsequence (LCS), weighted LCS, and skip-bigram co-occurence statistics, respectively. Instead of using only recall, these use an F-score which is the harmonic mean of precision and recall values. These are in turn, calculated as follows for ROUGE-L.
				<br/><br/>Suppose A and B are candidate and reference summaries of lengths m and n respectively. Then, we have
				<br/><br/><img img class='imgPos' src="../static/views/rog1.gif" alt="rogue">
				<br/><br/>F is then calculated as the weighted harmonic mean of P and R, as
				<br/><br/><img img class='imgPos' src="../static/views/rog2.gif" alt="rogue">
				<br/><br/>Similarly, in ROUGE-W, for calculating weighted LCS, we also track the lengths of the consecutive matches, in addition to the length of longest common subsequence (since there may be non-matching words in the middle). In ROUGE-S, a skip-bigram refers to any pair of words in sentence order allowing for arbitrary gaps. The precision and recall, in this case, are computed as a ratio of the total number of possible bigrams, i.e., C(n,2), where C is the combination function.
				<br/><br/><span style="font-weight: bold; font-size: 1.2vw;">3. METEOR (Metric for Evaluation for Translation with Explicit Ordering)</span>
				<br/>METEOR is another metric for machine translation evaluation, and it claims to have better correlation with human judgement.
				<br/><br/>So why do we need a new metric when BLEU is already available? The problem with BLEU is that since the BP value uses lengths which are averaged over the entire corpus, so the scores of individual sentences take a hit.
				<br/><br/>To solve this problem, METEOR modifies the precision and recall computations, replacing them with a weighted F-score based on mapping unigrams and a penalty function for incorrect word order.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">Weighted F-score: </span>
				<br/><br/>First, we try to find the largest subset of mappings that can form an alignment between the candidate and reference translations. For this, we look at exact matches, followed by matches after Porter stemming, and finally using WordNet synonymy. After such an alignment is found, suppose m is the number of mapped unigrams between the two texts. Then, precision and recall are given as m/c and m/r, where c and r are candidate and reference lengths, respectively. F is calculated as
				<br/><br/><img img class='imgPos' src="../static/views/met1.gif" alt="meteor">
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">Penalty function:</span>
				<br/><br/>To account for the word order in the candidate, we introduce a penalty function as
				<br/><br/><img img class='imgPos' src="../static/views/met2.gif" alt="meteor">
				<br/><br/>Here, c is the number of matching chunks and m is the total number of matches. As such, if most of the matches are contiguous, the number of chunks is lower and the penalty decreases. Finally, the METEOR score is given as (1-Penalty)F.
			</p>
		</div>
		<div id="section19">
			<a class="topic" style="text-decoration: underline;">Attention</a><br/>
			<p class="basicText2">
				<br/><span style="font-weight: bold; font-size: 1.2vw;">The Basic Idea: </span>
				<br/>Each time the model predicts an output word, it only uses parts of an input where the most relevant information is concentrated instead of an entire sentence. In other words, it only pays attention to some input words. Let’s investigate how this is implemented.
				<br/><br/><img img class='imgPos' src="../static/views/att1.png" alt="attention">
				<br/><br/>Encoder works as usual, and the difference is only on the decoder’s part. As you can see from a picture, the decoder’s hidden state is computed with a context vector, the previous output and the previous hidden state. But now we use not a single context vector c, but a separate context vector c_i for each target word.
				<br/><br/>These context vectors are computed as a weighted sum of annotations generated by the encoder. In Bahdanau’s paper, they use a Bidirectional LSTM, so these annotations are concatenations of hidden states in forward and backward directions.
				<br/><br/>The weight of each annotation is computed by an alignment model which scores how well the inputs and the output match. An alignment model is a feedforward neural network, for instance. In general, it can be any other model as well.
				<br/><br/>As a result, the alphas — the weights of hidden states when computing a context vector — show how important a given annotation is in deciding the next state and generating the output word. These are the attention scores.
				<br/><br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Memory Networks: </span>
				<br/><br/>One group of attention mechanisms repeats the computation of an attention vector between the query and the context through multiple layers. It is referred to as multi-hop. They are mainly variants of end-to-end memory networks, which we will discuss now.
				<br/><br/>[Sukhbaatar, 2015] argues that the attention mechanism implemented by Bahdanau can be seen as a form of memory. They extend this mechanism to a multi-hop setting. It means that the network reads the same input sequence multiple times before producing an output, and updates the memory contents at each step. Another modification is that the model works with multiple source sentences instead of a single one.
				<br/><br/><img img class='imgPos' src="../static/views/mn1.png" alt="memory network">
				<br/><br/>Let’s take a look at the inner workings. First, let me describe the single layer case (a). It implements a single memory hop operation. The entire input set of sentences is converted into memory vectors m. The query q is also embedded to obtain an internal state u. We compute the match between u and each memory by taking the inner product followed by a softmax. This way we obtain a probability vector p over the inputs (this is the attention part). Each input also has a corresponding output vector. We use the weights p to weigh a sum of these output vectors. This sum is a response vector o from the memory. Now we have an output vector o and the input embedding u. We sum them, multiply by a weight matrix W and apply a softmax to predict a label.
				<br/><br/>Now, we can extend the model to handle K hop operations (b). The memory layers are stacked so that the input to layers k + 1 is the sum of the output and the input from layer k. Each layer has its own embedding matrices for the inputs.
				<br/><br/>When the input and output embeddings are the same across different layers, the memory is identical to the attention mechanism of Bahdanau. The difference is that it makes multiple hops over the memory (because it tries to integrate information from multiple sentences).
				<br/><br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Variations of attention: </span>
				<br/><br/>[Luong, 2015] introduces the difference between <span style="font-weight: bold; font-size: 1.0vw;">global</span> and <span style="font-weight: bold; font-size: 1.0vw;">local</span> attention. The idea of a global attention is to use all the hidden states of the encoder when computing each context vector. The downside of a global attention model is that it has to attend to all words on the source side for each target word, which is computationally costly. To overcome this, the local attention first chooses a position in the source sentence. This position will determine a window of words that the model attends to. The authors also experimented with different alignment functions and simplified the computation path compared to Bahdanau’s work.
				<br/><br/>Attention Sum Reader [Kadlec, 2016] uses attention as a pointer over discrete tokens in the text. The task is to select an answer to a given question from the context paragraph. The difference with other methods is that the model selects the answer from the context directly using the computed attention instead of using the attention scores to weigh the sum of hidden vectors.
				<br/><br/><img img class='imgPos' src="../static/views/att2.png" alt="attention">
				<br/><br/>As an example, let us consider the question-context pair. Let the context be “A UFO was observed above our city in January and again in March.” and the question be “An observer has spotted a UFO in … .” January and March are equally good candidates, so the previous models will assign equal attention scores. They would then compute a vector between the representations of these two words and propose the word with the closest word embedding as the answer. At the same time, Attention Sum Reader would correctly propose January or March, because it chooses words directly from the passage.
				<br/><br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Two-way Attention & Coattention:</span>
				<br/><br/>As you might have noticed, in the previous model we pay attention from source to target. It makes sense in translation, but what about other fields? For example, consider textual entailment. We have a premise “If you help the needy, God will reward you” and a hypothesis “Giving money to a poor man has good consequences”. Our task is to understand whether the premise entails the hypothesis (in this case, it does). It would be useful to pay attention not only from the hypothesis to the text but also the other way around.
				<br/><br/>This brings the concept of two-way attention [Rocktäschel, 2015]. The idea is to use the same model to attend over the premise, as well as over the hypothesis. In the simplest form, you can simply swap the two sequences. This produces two attended representations which can be concatenated.
				<br/><br/>However, such a model will not let you emphasize more important matching results. For instance, alignment between stop words is less important than between the content words. In addition, the model still uses a single vector to represent the premise. To overcome these limitations, [Wang, Jiang, 2016] developed MatchLSTM. To deal with the importance of the matching, they add a special LSTM that will remember important matching results, while forgetting the others. This additional LSTM is also used to increase the granularity level. We will now multiply attention weights with each hidden state. It performed well in question answering and textual entailment tasks.
				<br/><br/>The question answering task gave rise to even more advanced ways to combine both sides. Bahdanau’s model, that we have seen in the beginning, uses a summary vector of the query to attend to the context. In contrast to it, the coattention is computed as an alignment matrix on all pairs of context and query words. As an example of this approach, let’s examine Dynamic Coattention Networks [Xiong, 2016].
				<br/><br/><img img class='imgPos' src="../static/views/att3.png" alt="attention">
				<br/><br/>Let’s walk through what is going on in the picture. First, we compute the affinity matrix of all pairs of document and question words. Then we get the attention weights AQ across the document for each word in the question and AD — the other way around. Next, the summary or attention context of the document in light of each word in the question is computed. In the same way, we can compute it for the question in light of each word in the document. Finally, we compute the summaries of the previous attention contexts given each word in the document. The resulting vectors are concatenated into a co-dependent representation of the question and the document. This is called the coattention context.
				<br/><br/><br/><span style="font-weight: bold; font-size: 1.2vw;">Self-attention:</span>
				<br/><br/>Another problem is that recurrent network can only memorize limited passage contexts in practice despite its theoretical capabilities. For example, in question answering, one answer candidate is often unaware of the clues in other parts of the paragraph. [Cheng, 2016] proposed a self-attention, sometimes called intra-attention. It is a mechanism relating different positions of a single sequence to compute its internal representation.
				<br/><br/>Self-attention has been used successfully in a variety of tasks. One of the use cases is sentiment analysis. For tasks like this, standard attention is not directly applicable, because there is no extra information: the model is only given one sentence as input. A common approach is to use the final hidden state or pooling. However, it is hard to preserve semantics this way. In Question Answering, there are models used in the SQuAD competition: r-net [Wang, 2017] and Reinforced Mnemonic Reader [Hu, 2017]. In Natural Language Inference — Decomposable Attention Model [Parikh, 2016]. 
				<br/><br/>In machine translation, self-attention also contributes to impressive results. For example, recently a model, named Transformer, was introduced in a paper with a rather bold title “Attention Is All You Need” [Vaswani, 2017]. As you can guess, this model relies only on self-attention without the use of RNNs. As a result, it is highly parallelizable and requires less time to train, while establishing state-of-the-art results on WMT2014.
				<br/><br/><img img class='imgPos' src="../static/views/att4.png" alt="attention">
			</p>
		</div>
	</section>
</body>
</html>
