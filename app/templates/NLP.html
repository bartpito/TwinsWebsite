<!DOCTYPE html></script>
<html>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
<head>
	<title>NLP</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link href="{{ url_for('static', filename='bootstrap/bootstrap.css') }}" rel="stylesheet" type="text/css">
	<link href="{{ url_for('static', filename='CSS/NLP.css') }}" rel="stylesheet" type="text/css">
	<script>
		$(document).ready(function(){
		// Add smooth scrolling to all links
		$("a").on('click', function(event) {
 
			// Make sure this.hash has a value before overriding default behavior
			if (this.hash !== "") {
			// Prevent default anchor click behavior
			event.preventDefault();
 
			// Store hash
			var hash = this.hash;
 
			// Using jQuery's animate() method to add smooth page scroll
			// The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
			$('html, body').animate({
				scrollTop: $(hash).offset().top
			}, 800, function(){
		
				// Add hash (#) to URL when done scrolling (default click behavior)
				window.location.hash = hash;
			});
			} // End if
		});
		});
	</script>
</head>
<body>
	<section class="app">
		<aside class="sidebar">
			<header style="font-weight: 999;" class="sidebar-navig">
				Natural Language Processing
			</header>
			<nav class="sidebar-nav">
				<ul>
					<li>
						<a href="#"><i class="ion-bag"></i> <span class="topic-decoration">&bull; Basic NLP</span></a>
						<ul class="nav-flyout">
							<li>
								<a href="#section1"><i class="ion-ios-color-filter-outline"></i>Data Augmentation</a>
							</li>
							<li>
								<a href="#section2"><i class="ion-ios-clock-outline"></i>Tokenization</a>
							</li>
							<li>
								<a href="#section3"><i class="ion-ios-color-filter-outline"></i>Text Preprocessing</a>
							</li>
						 	<li>
								<a href="#section4"><i class="ion-ios-clock-outline"></i>SVM</a>
							</li>
							<li>
								<a href="#section5"><i class="ion-ios-color-filter-outline"></i>SGD</a>
							</li>
							<li>
								<a href="#section6"><i class="ion-ios-clock-outline"></i>Logistic Regression</a>
							</li>
						</ul>
					</li>
					<li>
						<a href="#"><i class="ion-bag"></i> <span class="topic-decoration">&bull; NLTK</span></a>
						<ul class="nav-flyout">
							<li>
								<a href="#section1"><i class="ion-ios-color-filter-outline"></i>Data Augmentation</a>
							</li>
							<li>
								<a href="#section2"><i class="ion-ios-clock-outline"></i>Naive Bayes</a>
							</li>
							<li>
								<a href="#section3"><i class="ion-ios-color-filter-outline"></i>KNN</a>
							</li>
						 	<li>
								<a href="#section4"><i class="ion-ios-clock-outline"></i>SVM</a>
							</li>
							<li>
								<a href="#section5"><i class="ion-ios-color-filter-outline"></i>SGD</a>
							</li>
							<li>
								<a href="#section6"><i class="ion-ios-clock-outline"></i>Logistic Regression</a>
							</li>
						</ul>
					 </li>
					<li>
						 <a href="#"><i class="ion-bag"></i> <span class="topic-decoration">&bull; Neural Networks</span></a>
						 <ul class="nav-flyout">
							<li>
								<a href="#section11"><i class="ion-ios-clock-outline"></i>Learning process of Neural Network (NN)</a>
							</li>
							<li>
								<a href="#section12"><i class="ion-ios-color-filter-outline"></i>Keras Layers</a>
							</li>
							<li>
								<a href="#section13"><i class="ion-ios-clock-outline"></i>Convolutional NN</a>
							</li>
							<li>
								<a href="#section14"><i class="ion-ios-color-filter-outline"></i>Recurrent NN</a>
							</li>
							<li>
								<a href="#section15"><i class="ion-ios-color-filter-outline"></i>GANs</a>
							</li>
					 </ul>
					</li>
				</ul>
				<ul>
					<h2></h2>
					<li>
						<a href="{{ url_for( 'index' ) }}"><i class="ion-bag"></i> <span style="color: #FFCF75">&#8227 Home Page</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'about', name='pito' ) }}"><i class="ion-ios-settings"></i> <span style="color: #FFCF75">&#8227 About Pito</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'about', name='bart' ) }}"><i class="ion-ios-settings"></i> <span style="color: #FFCF75">&#8227 About Bart</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'projectsPage' ) }}"><i class="ion-ios-settings"></i> <span style="color: #FFCF75">&#8227 Projects Page</span></a>
					</li>
					<li>
						<a href="{{ url_for( 'learningPage' ) }}"><i class="ion-ios-settings"></i> <span style="color: #FFCF75">&#8227 Learning Page</span></a>
					</li>
				</ul>
			</nav>
		</aside>
		<div id="section1">
			<br/><a class="topic" style="text-decoration: underline;">Data Augmentation</a><br/>
			<br/><h2 class="basicText">1. Easiest Data Augmentation Techniques in Natural Language Processing:</h2>
			<p class="basicText2">
				<br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">Synonym Replacement:</span><br/> Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.
				<br/><br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">Random Insertion:</span> <br/>Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times.
				<br/><br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">Random Swap:</span> <br/>Randomly choose two words in the sentence and swap their positions. Do this n times.
				<br/><br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">Random Deletion:</span> <br/>Randomly remove each word in the sentence with probability p.
				<br/><br/><a href="https://github.com/jasonwei20/eda_nlp/blob/master/code/eda.py" style="font-weight: bold; text-decoration: underline; color: black; font-size: 1.2vw;">Code Implementation</a>
			</p>
		</div>
		<div id="section2">
			<br/><a class="topic" style="text-decoration: underline;">Tokenization</a><br/>
			<br/><h2 class="basicText">1. Description:</h2>
			<p class="basicText2">
				<br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">The process of segmenting running text into words and sentences.</span>
				Electronic text is a linear sequence of symbols (characters or words or phrases). Naturally, before any real text processing is to be done, text needs to be segmented into linguistic units such as words, punctuation, numbers, alpha-numerics, etc. This process is called tokenization.
				In English, words are often separated from each other by blanks (white space), but not all white space is equal. Both “Los Angeles” and “rock 'n' roll” are individual thoughts despite the fact that they contain multiple words and spaces. We may also need to separate single words like “I'm” into separate words “I” and “am”.
				Tokenization is a kind of pre-processing in a sense; an identification of basic units to be processed. It is conventional to concentrate on pure analysis or generation while taking basic units for granted. Yet without these basic units clearly segregated it is impossible to carry out any analysis or generation.
				The identification of units that do not need to be further decomposed for subsequent processing is an extremely important one. Errors made at this stage are very likely to induce more errors at later stages of text processing and are therefore very dangerous.
				<br/><br/><br/>&bull; <span style="font-weight: bold; font-size: 1.0vw;">More information can be found at this link:</span>
				<br/><br/><a href="https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en" style="font-weight: bold; text-decoration: underline; color: black; font-size: 1.2vw;">The Art of Tokenization</a>
			</p>
		</div>
		<div id="section3">
			<br/><a class="topic" style="text-decoration: underline;">Text Preprocessing:</a><br/>
			<br/><h2 class="basicText">What is text preprocessing?</h2>
			<p class="basicText2">
				<br/>&bull; To preprocess your text simply means to bring your text into a form that is <span style="font-weight: bold; font-size: 1.0vw;">predictable</span> and <span style="font-weight: bold; font-size: 1.0vw;">analyzable</span> for your task. A task here is a combination of approach and domain. For example, extracting 
				top keywords with TF-IDF (approach) from Tweets (domain) is an example of a Task.
			</p>
			<br/><h2 class="basicText">Types of text preprocessing techniques:</h2>
			<p class="basicText2">
				<span style="font-weight: bold; font-size: 1.1vw;">&bull; Lowercasing</span>
				<br/>Lowercasing ALL your text data, although commonly overlooked, is one of the simplest and most effective form of text preprocessing. It is applicable to most text mining and NLP problems and can help in cases where your dataset is not very large and significantly helps with consistency of expected output.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Stemming</span>
				<br/>Stemming is the process of reducing inflection in words (e.g. troubled, troubles) to their root form (e.g. trouble). The “root” in this case may not be a real root word, but just a canonical form of the original word.
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Lemmatization</span>
				<br/><p class="basicText2">

				</p>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Stemming</span>
				<br/><p class="basicText2">
					
				</p>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Stemming</span>
				<br/><p class="basicText2">
					
				</p>
				<br/><br/><span style="font-weight: bold; font-size: 1.1vw;">&bull; Stemming</span>
			</p>
		</div>
	</section>
</body>
</html>